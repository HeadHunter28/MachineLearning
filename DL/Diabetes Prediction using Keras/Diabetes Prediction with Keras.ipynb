{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Prediction in Keras\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all required libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import skillsnetwork\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models  import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset and loading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3fe295c5b8480497ac15b90f5f6df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading diabetes.csv:   0%|          | 0/23873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module2/L2/diabetes.csv\", overwrite=True)\n",
    "\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv('./diabetes.csv', names=names, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Taking a peek at the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>8</td>\n",
       "      <td>133</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.270</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>78</td>\n",
       "      <td>63</td>\n",
       "      <td>14</td>\n",
       "      <td>59.4</td>\n",
       "      <td>2.420</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>84</td>\n",
       "      <td>29</td>\n",
       "      <td>215</td>\n",
       "      <td>30.7</td>\n",
       "      <td>0.520</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>7</td>\n",
       "      <td>160</td>\n",
       "      <td>54</td>\n",
       "      <td>32</td>\n",
       "      <td>175</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.588</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>99</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.496</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "61                8                     133              72               0   \n",
       "445               0                     180              78              63   \n",
       "297               0                     126              84              29   \n",
       "114               7                     160              54              32   \n",
       "125               1                      88              30              42   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "61         0  32.9              0.270   39             1  \n",
       "445       14  59.4              2.420   25             1  \n",
       "297      215  30.7              0.520   24             0  \n",
       "114      175  30.5              0.588   39             1  \n",
       "125       99  55.0              0.496   26             1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting to input and target data, and training/testing data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RandomForestClassifier as a baseline prediction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions and getting the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.766\n",
      "roc-auc is 0.823\n"
     ]
    }
   ],
   "source": [
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a Sequential model to quickly build a neural network.\n",
    "Our first network will be a single layer network.  We have 8 variables, so input shape is set to 8. The hidden \n",
    "layer has 12 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final layer has just one node with a sigmoid activation (standard for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (8,),activation = 'sigmoid'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model for 200 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit function returns run history for each epoch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3075: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      " 32/576 [>.............................] - ETA: 2s - loss: 0.6390 - acc: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 16:41:57.701431: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2024-01-22 16:41:57.705646: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394310000 Hz\n",
      "2024-01-22 16:41:57.706293: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d3914769b0 executing computations on platform Host. Devices:\n",
      "2024-01-22 16:41:57.706349: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2024-01-22 16:41:57.752373: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 426us/step - loss: 0.6632 - acc: 0.6545 - val_loss: 0.6751 - val_acc: 0.6458\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.6622 - acc: 0.6545 - val_loss: 0.6740 - val_acc: 0.6458\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.6612 - acc: 0.6545 - val_loss: 0.6729 - val_acc: 0.6458\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.6603 - acc: 0.6545 - val_loss: 0.6718 - val_acc: 0.6458\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 125us/step - loss: 0.6593 - acc: 0.6545 - val_loss: 0.6708 - val_acc: 0.6458\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.6584 - acc: 0.6545 - val_loss: 0.6698 - val_acc: 0.6458\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.6576 - acc: 0.6545 - val_loss: 0.6688 - val_acc: 0.6458\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6567 - acc: 0.6545 - val_loss: 0.6678 - val_acc: 0.6458\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 104us/step - loss: 0.6558 - acc: 0.6545 - val_loss: 0.6669 - val_acc: 0.6458\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.6550 - acc: 0.6545 - val_loss: 0.6660 - val_acc: 0.6458\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6541 - acc: 0.6545 - val_loss: 0.6650 - val_acc: 0.6458\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.6533 - acc: 0.6545 - val_loss: 0.6641 - val_acc: 0.6458\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6525 - acc: 0.6545 - val_loss: 0.6633 - val_acc: 0.6458\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.6517 - acc: 0.6545 - val_loss: 0.6624 - val_acc: 0.6458\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.6509 - acc: 0.6545 - val_loss: 0.6615 - val_acc: 0.6458\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.6501 - acc: 0.6545 - val_loss: 0.6607 - val_acc: 0.6458\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.6493 - acc: 0.6545 - val_loss: 0.6598 - val_acc: 0.6458\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.6486 - acc: 0.6545 - val_loss: 0.6590 - val_acc: 0.6458\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.6478 - acc: 0.6545 - val_loss: 0.6582 - val_acc: 0.6458\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.6471 - acc: 0.6545 - val_loss: 0.6574 - val_acc: 0.6458\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.6463 - acc: 0.6545 - val_loss: 0.6566 - val_acc: 0.6458\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.6456 - acc: 0.6545 - val_loss: 0.6558 - val_acc: 0.6458\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.6449 - acc: 0.6545 - val_loss: 0.6550 - val_acc: 0.6458\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.6441 - acc: 0.6545 - val_loss: 0.6542 - val_acc: 0.6458\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6434 - acc: 0.6545 - val_loss: 0.6534 - val_acc: 0.6458\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6427 - acc: 0.6545 - val_loss: 0.6527 - val_acc: 0.6458\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.6420 - acc: 0.6545 - val_loss: 0.6519 - val_acc: 0.6458\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6413 - acc: 0.6545 - val_loss: 0.6512 - val_acc: 0.6458\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6406 - acc: 0.6545 - val_loss: 0.6504 - val_acc: 0.6458\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6399 - acc: 0.6545 - val_loss: 0.6497 - val_acc: 0.6458\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.6392 - acc: 0.6545 - val_loss: 0.6490 - val_acc: 0.6458\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.6385 - acc: 0.6545 - val_loss: 0.6482 - val_acc: 0.6458\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6378 - acc: 0.6545 - val_loss: 0.6475 - val_acc: 0.6458\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6371 - acc: 0.6545 - val_loss: 0.6468 - val_acc: 0.6458\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6365 - acc: 0.6545 - val_loss: 0.6461 - val_acc: 0.6458\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6358 - acc: 0.6545 - val_loss: 0.6454 - val_acc: 0.6458\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6351 - acc: 0.6545 - val_loss: 0.6447 - val_acc: 0.6458\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6345 - acc: 0.6545 - val_loss: 0.6440 - val_acc: 0.6458\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6338 - acc: 0.6545 - val_loss: 0.6433 - val_acc: 0.6458\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.6331 - acc: 0.6545 - val_loss: 0.6426 - val_acc: 0.6458\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.6325 - acc: 0.6545 - val_loss: 0.6419 - val_acc: 0.6458\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6318 - acc: 0.6545 - val_loss: 0.6413 - val_acc: 0.6458\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6312 - acc: 0.6545 - val_loss: 0.6406 - val_acc: 0.6458\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6306 - acc: 0.6545 - val_loss: 0.6399 - val_acc: 0.6458\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6300 - acc: 0.6545 - val_loss: 0.6393 - val_acc: 0.6458\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6293 - acc: 0.6545 - val_loss: 0.6386 - val_acc: 0.6458\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6286 - acc: 0.6545 - val_loss: 0.6379 - val_acc: 0.6458\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6280 - acc: 0.6545 - val_loss: 0.6373 - val_acc: 0.6458\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6274 - acc: 0.6545 - val_loss: 0.6366 - val_acc: 0.6458\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.6267 - acc: 0.6545 - val_loss: 0.6360 - val_acc: 0.6458\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.6261 - acc: 0.6545 - val_loss: 0.6354 - val_acc: 0.6458\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6255 - acc: 0.6545 - val_loss: 0.6347 - val_acc: 0.6458\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6249 - acc: 0.6545 - val_loss: 0.6341 - val_acc: 0.6458\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6243 - acc: 0.6545 - val_loss: 0.6334 - val_acc: 0.6458\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6237 - acc: 0.6545 - val_loss: 0.6328 - val_acc: 0.6458\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.6231 - acc: 0.6545 - val_loss: 0.6322 - val_acc: 0.6458\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6225 - acc: 0.6545 - val_loss: 0.6316 - val_acc: 0.6458\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6219 - acc: 0.6545 - val_loss: 0.6310 - val_acc: 0.6458\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6213 - acc: 0.6545 - val_loss: 0.6303 - val_acc: 0.6458\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6207 - acc: 0.6562 - val_loss: 0.6297 - val_acc: 0.6458\n",
      "Epoch 61/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6201 - acc: 0.6562 - val_loss: 0.6291 - val_acc: 0.6458\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6195 - acc: 0.6562 - val_loss: 0.6285 - val_acc: 0.6458\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6189 - acc: 0.6562 - val_loss: 0.6279 - val_acc: 0.6458\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.6183 - acc: 0.6562 - val_loss: 0.6273 - val_acc: 0.6458\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.6177 - acc: 0.6580 - val_loss: 0.6267 - val_acc: 0.6458\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.6171 - acc: 0.6580 - val_loss: 0.6261 - val_acc: 0.6458\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6166 - acc: 0.6580 - val_loss: 0.6255 - val_acc: 0.6458\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.6160 - acc: 0.6580 - val_loss: 0.6249 - val_acc: 0.6510\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6154 - acc: 0.6580 - val_loss: 0.6244 - val_acc: 0.6510\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6149 - acc: 0.6580 - val_loss: 0.6238 - val_acc: 0.6510\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6143 - acc: 0.6580 - val_loss: 0.6232 - val_acc: 0.6510\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.6137 - acc: 0.6580 - val_loss: 0.6226 - val_acc: 0.6510\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6132 - acc: 0.6562 - val_loss: 0.6220 - val_acc: 0.6510\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6126 - acc: 0.6562 - val_loss: 0.6215 - val_acc: 0.6510\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6120 - acc: 0.6562 - val_loss: 0.6209 - val_acc: 0.6510\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6115 - acc: 0.6562 - val_loss: 0.6203 - val_acc: 0.6510\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6109 - acc: 0.6562 - val_loss: 0.6198 - val_acc: 0.6510\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.6103 - acc: 0.6562 - val_loss: 0.6192 - val_acc: 0.6510\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.6098 - acc: 0.6562 - val_loss: 0.6186 - val_acc: 0.6510\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6093 - acc: 0.6562 - val_loss: 0.6181 - val_acc: 0.6510\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.6087 - acc: 0.6562 - val_loss: 0.6175 - val_acc: 0.6510\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6082 - acc: 0.6562 - val_loss: 0.6170 - val_acc: 0.6562\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6076 - acc: 0.6580 - val_loss: 0.6164 - val_acc: 0.6562\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.6071 - acc: 0.6580 - val_loss: 0.6159 - val_acc: 0.6562\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6065 - acc: 0.6580 - val_loss: 0.6153 - val_acc: 0.6562\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6060 - acc: 0.6580 - val_loss: 0.6148 - val_acc: 0.6562\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6055 - acc: 0.6597 - val_loss: 0.6142 - val_acc: 0.6562\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.6049 - acc: 0.6597 - val_loss: 0.6137 - val_acc: 0.6562\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6044 - acc: 0.6597 - val_loss: 0.6131 - val_acc: 0.6562\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6039 - acc: 0.6615 - val_loss: 0.6126 - val_acc: 0.6562\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6033 - acc: 0.6615 - val_loss: 0.6121 - val_acc: 0.6562\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.6028 - acc: 0.6562 - val_loss: 0.6116 - val_acc: 0.6562\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6023 - acc: 0.6580 - val_loss: 0.6110 - val_acc: 0.6562\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.6018 - acc: 0.6580 - val_loss: 0.6105 - val_acc: 0.6562\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6013 - acc: 0.6580 - val_loss: 0.6100 - val_acc: 0.6562\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6008 - acc: 0.6597 - val_loss: 0.6094 - val_acc: 0.6562\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6002 - acc: 0.6597 - val_loss: 0.6089 - val_acc: 0.6562\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5997 - acc: 0.6580 - val_loss: 0.6084 - val_acc: 0.6562\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5993 - acc: 0.6580 - val_loss: 0.6079 - val_acc: 0.6562\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5987 - acc: 0.6580 - val_loss: 0.6074 - val_acc: 0.6615\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5982 - acc: 0.6580 - val_loss: 0.6069 - val_acc: 0.6615\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5977 - acc: 0.6597 - val_loss: 0.6064 - val_acc: 0.6615\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5972 - acc: 0.6597 - val_loss: 0.6059 - val_acc: 0.6615\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5967 - acc: 0.6597 - val_loss: 0.6053 - val_acc: 0.6615\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5962 - acc: 0.6597 - val_loss: 0.6048 - val_acc: 0.6615\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5957 - acc: 0.6615 - val_loss: 0.6043 - val_acc: 0.6615\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5952 - acc: 0.6615 - val_loss: 0.6038 - val_acc: 0.6615\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5947 - acc: 0.6615 - val_loss: 0.6033 - val_acc: 0.6615\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5942 - acc: 0.6615 - val_loss: 0.6028 - val_acc: 0.6615\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5937 - acc: 0.6615 - val_loss: 0.6024 - val_acc: 0.6667\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5933 - acc: 0.6632 - val_loss: 0.6019 - val_acc: 0.6667\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5928 - acc: 0.6615 - val_loss: 0.6014 - val_acc: 0.6667\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5923 - acc: 0.6615 - val_loss: 0.6009 - val_acc: 0.6667\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5918 - acc: 0.6615 - val_loss: 0.6004 - val_acc: 0.6615\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5913 - acc: 0.6597 - val_loss: 0.5999 - val_acc: 0.6615\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5908 - acc: 0.6580 - val_loss: 0.5994 - val_acc: 0.6667\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5904 - acc: 0.6597 - val_loss: 0.5989 - val_acc: 0.6667\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5899 - acc: 0.6597 - val_loss: 0.5985 - val_acc: 0.6667\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5894 - acc: 0.6580 - val_loss: 0.5980 - val_acc: 0.6667\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5889 - acc: 0.6580 - val_loss: 0.5975 - val_acc: 0.6667\n",
      "Epoch 121/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5884 - acc: 0.6580 - val_loss: 0.5970 - val_acc: 0.6667\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5880 - acc: 0.6580 - val_loss: 0.5966 - val_acc: 0.6667\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5875 - acc: 0.6580 - val_loss: 0.5961 - val_acc: 0.6667\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5870 - acc: 0.6580 - val_loss: 0.5956 - val_acc: 0.6667\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5866 - acc: 0.6597 - val_loss: 0.5952 - val_acc: 0.6719\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5861 - acc: 0.6597 - val_loss: 0.5947 - val_acc: 0.6719\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5856 - acc: 0.6615 - val_loss: 0.5942 - val_acc: 0.6771\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5852 - acc: 0.6597 - val_loss: 0.5938 - val_acc: 0.6823\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5847 - acc: 0.6615 - val_loss: 0.5933 - val_acc: 0.6927\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5843 - acc: 0.6580 - val_loss: 0.5929 - val_acc: 0.6927\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5838 - acc: 0.6615 - val_loss: 0.5924 - val_acc: 0.6927\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5834 - acc: 0.6632 - val_loss: 0.5920 - val_acc: 0.6927\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5829 - acc: 0.6667 - val_loss: 0.5915 - val_acc: 0.6979\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5825 - acc: 0.6649 - val_loss: 0.5911 - val_acc: 0.6979\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5821 - acc: 0.6667 - val_loss: 0.5906 - val_acc: 0.6979\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5816 - acc: 0.6667 - val_loss: 0.5902 - val_acc: 0.6979\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5812 - acc: 0.6667 - val_loss: 0.5897 - val_acc: 0.6979\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5807 - acc: 0.6701 - val_loss: 0.5893 - val_acc: 0.6979\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5803 - acc: 0.6701 - val_loss: 0.5888 - val_acc: 0.6979\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5798 - acc: 0.6701 - val_loss: 0.5884 - val_acc: 0.6979\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5794 - acc: 0.6701 - val_loss: 0.5879 - val_acc: 0.6979\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5790 - acc: 0.6701 - val_loss: 0.5875 - val_acc: 0.6979\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5785 - acc: 0.6736 - val_loss: 0.5871 - val_acc: 0.6979\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5781 - acc: 0.6736 - val_loss: 0.5866 - val_acc: 0.6979\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5777 - acc: 0.6719 - val_loss: 0.5862 - val_acc: 0.6979\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5772 - acc: 0.6736 - val_loss: 0.5858 - val_acc: 0.7031\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5768 - acc: 0.6736 - val_loss: 0.5853 - val_acc: 0.7031\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5763 - acc: 0.6719 - val_loss: 0.5849 - val_acc: 0.7031\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5759 - acc: 0.6719 - val_loss: 0.5845 - val_acc: 0.7031\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5755 - acc: 0.6719 - val_loss: 0.5841 - val_acc: 0.7031\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5751 - acc: 0.6736 - val_loss: 0.5836 - val_acc: 0.7083\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5746 - acc: 0.6719 - val_loss: 0.5832 - val_acc: 0.7083\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5742 - acc: 0.6736 - val_loss: 0.5828 - val_acc: 0.7135\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5738 - acc: 0.6736 - val_loss: 0.5824 - val_acc: 0.7135\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5734 - acc: 0.6753 - val_loss: 0.5820 - val_acc: 0.7135\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5730 - acc: 0.6771 - val_loss: 0.5816 - val_acc: 0.7135\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5726 - acc: 0.6788 - val_loss: 0.5811 - val_acc: 0.7135\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5722 - acc: 0.6771 - val_loss: 0.5807 - val_acc: 0.7135\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5717 - acc: 0.6806 - val_loss: 0.5803 - val_acc: 0.7135\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5713 - acc: 0.6806 - val_loss: 0.5799 - val_acc: 0.7135\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5709 - acc: 0.6823 - val_loss: 0.5795 - val_acc: 0.7135\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5705 - acc: 0.6823 - val_loss: 0.5791 - val_acc: 0.7135\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5701 - acc: 0.6823 - val_loss: 0.5787 - val_acc: 0.7135\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5697 - acc: 0.6823 - val_loss: 0.5783 - val_acc: 0.7135\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5693 - acc: 0.6806 - val_loss: 0.5779 - val_acc: 0.7135\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5689 - acc: 0.6806 - val_loss: 0.5775 - val_acc: 0.7135\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5685 - acc: 0.6840 - val_loss: 0.5771 - val_acc: 0.7135\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5681 - acc: 0.6840 - val_loss: 0.5767 - val_acc: 0.7135\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5677 - acc: 0.6840 - val_loss: 0.5763 - val_acc: 0.7135\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5673 - acc: 0.6840 - val_loss: 0.5759 - val_acc: 0.7135\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5669 - acc: 0.6840 - val_loss: 0.5755 - val_acc: 0.7135\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5665 - acc: 0.6840 - val_loss: 0.5751 - val_acc: 0.7135\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5661 - acc: 0.6858 - val_loss: 0.5747 - val_acc: 0.7135\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5657 - acc: 0.6858 - val_loss: 0.5744 - val_acc: 0.7135\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5653 - acc: 0.6858 - val_loss: 0.5740 - val_acc: 0.7188\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5649 - acc: 0.6858 - val_loss: 0.5736 - val_acc: 0.7188\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5645 - acc: 0.6875 - val_loss: 0.5732 - val_acc: 0.7188\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5642 - acc: 0.6858 - val_loss: 0.5728 - val_acc: 0.7240\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5638 - acc: 0.6858 - val_loss: 0.5725 - val_acc: 0.7240\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5634 - acc: 0.6858 - val_loss: 0.5721 - val_acc: 0.7240\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5630 - acc: 0.6858 - val_loss: 0.5717 - val_acc: 0.7240\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5626 - acc: 0.6875 - val_loss: 0.5713 - val_acc: 0.7240\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5622 - acc: 0.6892 - val_loss: 0.5709 - val_acc: 0.7240\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5619 - acc: 0.6910 - val_loss: 0.5706 - val_acc: 0.7240\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5615 - acc: 0.6927 - val_loss: 0.5702 - val_acc: 0.7292\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5611 - acc: 0.6892 - val_loss: 0.5698 - val_acc: 0.7292\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5608 - acc: 0.6927 - val_loss: 0.5695 - val_acc: 0.7292\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5604 - acc: 0.6944 - val_loss: 0.5691 - val_acc: 0.7292\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5600 - acc: 0.6927 - val_loss: 0.5687 - val_acc: 0.7292\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5596 - acc: 0.6962 - val_loss: 0.5684 - val_acc: 0.7292\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5592 - acc: 0.6962 - val_loss: 0.5680 - val_acc: 0.7292\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5589 - acc: 0.6962 - val_loss: 0.5676 - val_acc: 0.7292\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5585 - acc: 0.6962 - val_loss: 0.5673 - val_acc: 0.7292\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5582 - acc: 0.6979 - val_loss: 0.5669 - val_acc: 0.7292\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5578 - acc: 0.6979 - val_loss: 0.5666 - val_acc: 0.7292\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5574 - acc: 0.7014 - val_loss: 0.5662 - val_acc: 0.7292\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5571 - acc: 0.7014 - val_loss: 0.5659 - val_acc: 0.7292\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5567 - acc: 0.7014 - val_loss: 0.5655 - val_acc: 0.7292\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5564 - acc: 0.6997 - val_loss: 0.5652 - val_acc: 0.7292\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5560 - acc: 0.7014 - val_loss: 0.5648 - val_acc: 0.7292\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generates two kinds of predictions, hard predictions and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3293754 ],\n",
       "       [0.45040524],\n",
       "       [0.33884022],\n",
       "       [0.4146525 ],\n",
       "       [0.24942726],\n",
       "       [0.38205898],\n",
       "       [0.23642203],\n",
       "       [0.35938677],\n",
       "       [0.551118  ],\n",
       "       [0.36254662]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking model accuracy and ROC curve :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.729\n",
      "roc-auc is 0.780\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the run_hist_1 object that was created, specifically its history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the training loss and the validation loss over the different epochs :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5d557e2cd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.\n",
    "Training the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.5556 - acc: 0.7014 - val_loss: 0.5645 - val_acc: 0.7292\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5553 - acc: 0.6997 - val_loss: 0.5641 - val_acc: 0.7292\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5549 - acc: 0.7014 - val_loss: 0.5638 - val_acc: 0.7292\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5546 - acc: 0.7014 - val_loss: 0.5634 - val_acc: 0.7292\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5542 - acc: 0.7014 - val_loss: 0.5631 - val_acc: 0.7292\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5539 - acc: 0.7066 - val_loss: 0.5627 - val_acc: 0.7344\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5535 - acc: 0.7066 - val_loss: 0.5624 - val_acc: 0.7344\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5532 - acc: 0.7066 - val_loss: 0.5621 - val_acc: 0.7344\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5528 - acc: 0.7083 - val_loss: 0.5617 - val_acc: 0.7344\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5525 - acc: 0.7083 - val_loss: 0.5614 - val_acc: 0.7344\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5521 - acc: 0.7083 - val_loss: 0.5610 - val_acc: 0.7344\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5518 - acc: 0.7101 - val_loss: 0.5607 - val_acc: 0.7344\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5515 - acc: 0.7101 - val_loss: 0.5604 - val_acc: 0.7344\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5511 - acc: 0.7101 - val_loss: 0.5600 - val_acc: 0.7344\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5507 - acc: 0.7135 - val_loss: 0.5597 - val_acc: 0.7344\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5504 - acc: 0.7135 - val_loss: 0.5594 - val_acc: 0.7344\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5501 - acc: 0.7135 - val_loss: 0.5591 - val_acc: 0.7344\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5498 - acc: 0.7101 - val_loss: 0.5587 - val_acc: 0.7344\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5494 - acc: 0.7118 - val_loss: 0.5584 - val_acc: 0.7344\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5491 - acc: 0.7118 - val_loss: 0.5581 - val_acc: 0.7344\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5488 - acc: 0.7135 - val_loss: 0.5578 - val_acc: 0.7344\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5484 - acc: 0.7135 - val_loss: 0.5574 - val_acc: 0.7344\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5481 - acc: 0.7135 - val_loss: 0.5571 - val_acc: 0.7344\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5478 - acc: 0.7135 - val_loss: 0.5568 - val_acc: 0.7344\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5474 - acc: 0.7170 - val_loss: 0.5565 - val_acc: 0.7344\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5471 - acc: 0.7188 - val_loss: 0.5562 - val_acc: 0.7396\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5468 - acc: 0.7205 - val_loss: 0.5559 - val_acc: 0.7396\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5465 - acc: 0.7205 - val_loss: 0.5555 - val_acc: 0.7396\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5461 - acc: 0.7240 - val_loss: 0.5552 - val_acc: 0.7396\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5458 - acc: 0.7222 - val_loss: 0.5549 - val_acc: 0.7344\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5455 - acc: 0.7257 - val_loss: 0.5546 - val_acc: 0.7292\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5452 - acc: 0.7257 - val_loss: 0.5543 - val_acc: 0.7292\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5448 - acc: 0.7274 - val_loss: 0.5540 - val_acc: 0.7292\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5445 - acc: 0.7274 - val_loss: 0.5537 - val_acc: 0.7344\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5442 - acc: 0.7274 - val_loss: 0.5534 - val_acc: 0.7344\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5439 - acc: 0.7240 - val_loss: 0.5531 - val_acc: 0.7344\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5436 - acc: 0.7240 - val_loss: 0.5528 - val_acc: 0.7344\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5433 - acc: 0.7240 - val_loss: 0.5525 - val_acc: 0.7344\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5430 - acc: 0.7257 - val_loss: 0.5522 - val_acc: 0.7344\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5426 - acc: 0.7257 - val_loss: 0.5519 - val_acc: 0.7344\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5423 - acc: 0.7257 - val_loss: 0.5516 - val_acc: 0.7344\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5420 - acc: 0.7257 - val_loss: 0.5513 - val_acc: 0.7292\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5417 - acc: 0.7257 - val_loss: 0.5510 - val_acc: 0.7292\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5414 - acc: 0.7257 - val_loss: 0.5507 - val_acc: 0.7292\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5411 - acc: 0.7257 - val_loss: 0.5504 - val_acc: 0.7292\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5408 - acc: 0.7257 - val_loss: 0.5501 - val_acc: 0.7292\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5405 - acc: 0.7274 - val_loss: 0.5498 - val_acc: 0.7292\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5402 - acc: 0.7257 - val_loss: 0.5495 - val_acc: 0.7292\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5399 - acc: 0.7257 - val_loss: 0.5492 - val_acc: 0.7240\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5396 - acc: 0.7257 - val_loss: 0.5489 - val_acc: 0.7344\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5393 - acc: 0.7274 - val_loss: 0.5487 - val_acc: 0.7396\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5390 - acc: 0.7274 - val_loss: 0.5484 - val_acc: 0.7396\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5387 - acc: 0.7274 - val_loss: 0.5481 - val_acc: 0.7396\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5384 - acc: 0.7274 - val_loss: 0.5478 - val_acc: 0.7396\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5381 - acc: 0.7274 - val_loss: 0.5475 - val_acc: 0.7396\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5378 - acc: 0.7274 - val_loss: 0.5472 - val_acc: 0.7396\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5375 - acc: 0.7274 - val_loss: 0.5470 - val_acc: 0.7396\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5372 - acc: 0.7274 - val_loss: 0.5467 - val_acc: 0.7396\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5369 - acc: 0.7257 - val_loss: 0.5464 - val_acc: 0.7396\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5366 - acc: 0.7274 - val_loss: 0.5461 - val_acc: 0.7396\n",
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5364 - acc: 0.7240 - val_loss: 0.5459 - val_acc: 0.7396\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5361 - acc: 0.7240 - val_loss: 0.5456 - val_acc: 0.7396\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5358 - acc: 0.7257 - val_loss: 0.5453 - val_acc: 0.7396\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5355 - acc: 0.7257 - val_loss: 0.5450 - val_acc: 0.7396\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5352 - acc: 0.7257 - val_loss: 0.5448 - val_acc: 0.7396\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5349 - acc: 0.7257 - val_loss: 0.5445 - val_acc: 0.7396\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5346 - acc: 0.7257 - val_loss: 0.5442 - val_acc: 0.7396\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5344 - acc: 0.7257 - val_loss: 0.5440 - val_acc: 0.7396\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5341 - acc: 0.7257 - val_loss: 0.5437 - val_acc: 0.7396\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5338 - acc: 0.7257 - val_loss: 0.5434 - val_acc: 0.7396\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5335 - acc: 0.7274 - val_loss: 0.5432 - val_acc: 0.7448\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5332 - acc: 0.7274 - val_loss: 0.5429 - val_acc: 0.7448\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5330 - acc: 0.7274 - val_loss: 0.5426 - val_acc: 0.7448\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5327 - acc: 0.7309 - val_loss: 0.5424 - val_acc: 0.7448\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5324 - acc: 0.7309 - val_loss: 0.5421 - val_acc: 0.7448\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5321 - acc: 0.7309 - val_loss: 0.5419 - val_acc: 0.7448\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5319 - acc: 0.7326 - val_loss: 0.5416 - val_acc: 0.7448\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5316 - acc: 0.7326 - val_loss: 0.5413 - val_acc: 0.7448\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5313 - acc: 0.7326 - val_loss: 0.5411 - val_acc: 0.7448\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5311 - acc: 0.7344 - val_loss: 0.5408 - val_acc: 0.7448\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5308 - acc: 0.7326 - val_loss: 0.5406 - val_acc: 0.7448\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5305 - acc: 0.7326 - val_loss: 0.5403 - val_acc: 0.7448\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5302 - acc: 0.7326 - val_loss: 0.5401 - val_acc: 0.7448\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5300 - acc: 0.7326 - val_loss: 0.5398 - val_acc: 0.7448\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5297 - acc: 0.7326 - val_loss: 0.5396 - val_acc: 0.7448\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5294 - acc: 0.7361 - val_loss: 0.5393 - val_acc: 0.7448\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5292 - acc: 0.7344 - val_loss: 0.5391 - val_acc: 0.7500\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5289 - acc: 0.7344 - val_loss: 0.5388 - val_acc: 0.7500\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5287 - acc: 0.7378 - val_loss: 0.5386 - val_acc: 0.7500\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5284 - acc: 0.7344 - val_loss: 0.5384 - val_acc: 0.7500\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5281 - acc: 0.7378 - val_loss: 0.5381 - val_acc: 0.7500\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5279 - acc: 0.7396 - val_loss: 0.5379 - val_acc: 0.7500\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5276 - acc: 0.7396 - val_loss: 0.5376 - val_acc: 0.7500\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5274 - acc: 0.7396 - val_loss: 0.5374 - val_acc: 0.7500\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5271 - acc: 0.7396 - val_loss: 0.5371 - val_acc: 0.7500\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5269 - acc: 0.7396 - val_loss: 0.5369 - val_acc: 0.7500\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5266 - acc: 0.7413 - val_loss: 0.5367 - val_acc: 0.7500\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5264 - acc: 0.7413 - val_loss: 0.5364 - val_acc: 0.7500\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5261 - acc: 0.7413 - val_loss: 0.5362 - val_acc: 0.7500\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5259 - acc: 0.7413 - val_loss: 0.5360 - val_acc: 0.7500\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5256 - acc: 0.7413 - val_loss: 0.5357 - val_acc: 0.7500\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5253 - acc: 0.7378 - val_loss: 0.5355 - val_acc: 0.7500\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5251 - acc: 0.7396 - val_loss: 0.5353 - val_acc: 0.7500\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5248 - acc: 0.7396 - val_loss: 0.5350 - val_acc: 0.7500\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5246 - acc: 0.7413 - val_loss: 0.5348 - val_acc: 0.7500\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5244 - acc: 0.7413 - val_loss: 0.5346 - val_acc: 0.7500\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5241 - acc: 0.7413 - val_loss: 0.5344 - val_acc: 0.7500\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5239 - acc: 0.7413 - val_loss: 0.5341 - val_acc: 0.7500\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5236 - acc: 0.7431 - val_loss: 0.5339 - val_acc: 0.7500\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5234 - acc: 0.7413 - val_loss: 0.5337 - val_acc: 0.7500\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5231 - acc: 0.7431 - val_loss: 0.5335 - val_acc: 0.7500\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5229 - acc: 0.7431 - val_loss: 0.5332 - val_acc: 0.7500\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5227 - acc: 0.7431 - val_loss: 0.5330 - val_acc: 0.7500\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5224 - acc: 0.7431 - val_loss: 0.5328 - val_acc: 0.7500\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5222 - acc: 0.7431 - val_loss: 0.5326 - val_acc: 0.7500\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5219 - acc: 0.7431 - val_loss: 0.5323 - val_acc: 0.7500\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5217 - acc: 0.7413 - val_loss: 0.5321 - val_acc: 0.7500\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5215 - acc: 0.7448 - val_loss: 0.5319 - val_acc: 0.7500\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5213 - acc: 0.7483 - val_loss: 0.5317 - val_acc: 0.7500\n",
      "Epoch 120/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5210 - acc: 0.7431 - val_loss: 0.5315 - val_acc: 0.7500\n",
      "Epoch 121/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5208 - acc: 0.7483 - val_loss: 0.5313 - val_acc: 0.7500\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5205 - acc: 0.7483 - val_loss: 0.5311 - val_acc: 0.7500\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5203 - acc: 0.7483 - val_loss: 0.5308 - val_acc: 0.7500\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5201 - acc: 0.7500 - val_loss: 0.5306 - val_acc: 0.7500\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5199 - acc: 0.7483 - val_loss: 0.5304 - val_acc: 0.7500\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5196 - acc: 0.7483 - val_loss: 0.5302 - val_acc: 0.7500\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5194 - acc: 0.7483 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5191 - acc: 0.7483 - val_loss: 0.5298 - val_acc: 0.7500\n",
      "Epoch 129/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5189 - acc: 0.7483 - val_loss: 0.5296 - val_acc: 0.7500\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5187 - acc: 0.7483 - val_loss: 0.5294 - val_acc: 0.7500\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5185 - acc: 0.7483 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5182 - acc: 0.7483 - val_loss: 0.5290 - val_acc: 0.7500\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5180 - acc: 0.7483 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5178 - acc: 0.7483 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5176 - acc: 0.7483 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5174 - acc: 0.7483 - val_loss: 0.5282 - val_acc: 0.7500\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5171 - acc: 0.7483 - val_loss: 0.5280 - val_acc: 0.7500\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5169 - acc: 0.7483 - val_loss: 0.5278 - val_acc: 0.7500\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5167 - acc: 0.7483 - val_loss: 0.5276 - val_acc: 0.7500\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5165 - acc: 0.7483 - val_loss: 0.5274 - val_acc: 0.7500\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5163 - acc: 0.7483 - val_loss: 0.5272 - val_acc: 0.7500\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5161 - acc: 0.7483 - val_loss: 0.5270 - val_acc: 0.7500\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5158 - acc: 0.7483 - val_loss: 0.5268 - val_acc: 0.7500\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5156 - acc: 0.7483 - val_loss: 0.5266 - val_acc: 0.7500\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5154 - acc: 0.7483 - val_loss: 0.5264 - val_acc: 0.7500\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5152 - acc: 0.7500 - val_loss: 0.5262 - val_acc: 0.7500\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5150 - acc: 0.7500 - val_loss: 0.5260 - val_acc: 0.7500\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5148 - acc: 0.7500 - val_loss: 0.5258 - val_acc: 0.7552\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5145 - acc: 0.7500 - val_loss: 0.5256 - val_acc: 0.7500\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5144 - acc: 0.7483 - val_loss: 0.5254 - val_acc: 0.7500\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5141 - acc: 0.7500 - val_loss: 0.5252 - val_acc: 0.7500\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5139 - acc: 0.7483 - val_loss: 0.5250 - val_acc: 0.7500\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5137 - acc: 0.7483 - val_loss: 0.5249 - val_acc: 0.7500\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5135 - acc: 0.7483 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5133 - acc: 0.7483 - val_loss: 0.5245 - val_acc: 0.7552\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5131 - acc: 0.7483 - val_loss: 0.5243 - val_acc: 0.7552\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5129 - acc: 0.7483 - val_loss: 0.5241 - val_acc: 0.7552\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5127 - acc: 0.7483 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5125 - acc: 0.7500 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5123 - acc: 0.7500 - val_loss: 0.5236 - val_acc: 0.7552\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5121 - acc: 0.7517 - val_loss: 0.5234 - val_acc: 0.7552\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5119 - acc: 0.7535 - val_loss: 0.5232 - val_acc: 0.7604\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5117 - acc: 0.7535 - val_loss: 0.5230 - val_acc: 0.7604\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5115 - acc: 0.7535 - val_loss: 0.5228 - val_acc: 0.7604\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5113 - acc: 0.7535 - val_loss: 0.5227 - val_acc: 0.7604\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5111 - acc: 0.7535 - val_loss: 0.5225 - val_acc: 0.7604\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5109 - acc: 0.7535 - val_loss: 0.5223 - val_acc: 0.7604\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5107 - acc: 0.7535 - val_loss: 0.5221 - val_acc: 0.7604\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5105 - acc: 0.7535 - val_loss: 0.5220 - val_acc: 0.7604\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5103 - acc: 0.7552 - val_loss: 0.5218 - val_acc: 0.7604\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5101 - acc: 0.7535 - val_loss: 0.5216 - val_acc: 0.7604\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5099 - acc: 0.7552 - val_loss: 0.5214 - val_acc: 0.7552\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5097 - acc: 0.7552 - val_loss: 0.5213 - val_acc: 0.7552\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5095 - acc: 0.7552 - val_loss: 0.5211 - val_acc: 0.7552\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5093 - acc: 0.7552 - val_loss: 0.5209 - val_acc: 0.7552\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5091 - acc: 0.7552 - val_loss: 0.5208 - val_acc: 0.7552\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5090 - acc: 0.7552 - val_loss: 0.5206 - val_acc: 0.7552\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5088 - acc: 0.7569 - val_loss: 0.5204 - val_acc: 0.7552\n",
      "Epoch 179/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5086 - acc: 0.7569 - val_loss: 0.5203 - val_acc: 0.7552\n",
      "Epoch 180/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5084 - acc: 0.7569 - val_loss: 0.5201 - val_acc: 0.7604\n",
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5082 - acc: 0.7569 - val_loss: 0.5199 - val_acc: 0.7604\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5080 - acc: 0.7569 - val_loss: 0.5198 - val_acc: 0.7604\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5078 - acc: 0.7569 - val_loss: 0.5196 - val_acc: 0.7656\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5076 - acc: 0.7569 - val_loss: 0.5194 - val_acc: 0.7656\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5074 - acc: 0.7569 - val_loss: 0.5193 - val_acc: 0.7656\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5073 - acc: 0.7552 - val_loss: 0.5191 - val_acc: 0.7656\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5071 - acc: 0.7552 - val_loss: 0.5189 - val_acc: 0.7656\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5069 - acc: 0.7552 - val_loss: 0.5188 - val_acc: 0.7656\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5067 - acc: 0.7552 - val_loss: 0.5186 - val_acc: 0.7656\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5065 - acc: 0.7552 - val_loss: 0.5185 - val_acc: 0.7656\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5064 - acc: 0.7552 - val_loss: 0.5183 - val_acc: 0.7656\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5062 - acc: 0.7552 - val_loss: 0.5181 - val_acc: 0.7656\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5060 - acc: 0.7552 - val_loss: 0.5180 - val_acc: 0.7656\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5058 - acc: 0.7552 - val_loss: 0.5178 - val_acc: 0.7604\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5056 - acc: 0.7552 - val_loss: 0.5177 - val_acc: 0.7604\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5055 - acc: 0.7552 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5053 - acc: 0.7552 - val_loss: 0.5173 - val_acc: 0.7604\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5051 - acc: 0.7569 - val_loss: 0.5172 - val_acc: 0.7604\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5049 - acc: 0.7587 - val_loss: 0.5170 - val_acc: 0.7604\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5048 - acc: 0.7569 - val_loss: 0.5169 - val_acc: 0.7604\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5046 - acc: 0.7569 - val_loss: 0.5167 - val_acc: 0.7604\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5044 - acc: 0.7569 - val_loss: 0.5166 - val_acc: 0.7604\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5042 - acc: 0.7569 - val_loss: 0.5164 - val_acc: 0.7604\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5041 - acc: 0.7569 - val_loss: 0.5163 - val_acc: 0.7604\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5039 - acc: 0.7587 - val_loss: 0.5161 - val_acc: 0.7604\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5037 - acc: 0.7587 - val_loss: 0.5160 - val_acc: 0.7552\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5036 - acc: 0.7587 - val_loss: 0.5158 - val_acc: 0.7552\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5034 - acc: 0.7587 - val_loss: 0.5157 - val_acc: 0.7552\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5032 - acc: 0.7569 - val_loss: 0.5155 - val_acc: 0.7552\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5030 - acc: 0.7569 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5029 - acc: 0.7569 - val_loss: 0.5152 - val_acc: 0.7552\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5027 - acc: 0.7569 - val_loss: 0.5151 - val_acc: 0.7552\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5026 - acc: 0.7569 - val_loss: 0.5149 - val_acc: 0.7552\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5024 - acc: 0.7569 - val_loss: 0.5148 - val_acc: 0.7552\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5022 - acc: 0.7569 - val_loss: 0.5147 - val_acc: 0.7552\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5020 - acc: 0.7569 - val_loss: 0.5145 - val_acc: 0.7552\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5019 - acc: 0.7569 - val_loss: 0.5144 - val_acc: 0.7552\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.5017 - acc: 0.7569 - val_loss: 0.5142 - val_acc: 0.7552\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5015 - acc: 0.7587 - val_loss: 0.5141 - val_acc: 0.7552\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5014 - acc: 0.7587 - val_loss: 0.5139 - val_acc: 0.7552\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5012 - acc: 0.7569 - val_loss: 0.5138 - val_acc: 0.7552\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5011 - acc: 0.7587 - val_loss: 0.5137 - val_acc: 0.7552\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5009 - acc: 0.7569 - val_loss: 0.5135 - val_acc: 0.7552\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5008 - acc: 0.7587 - val_loss: 0.5134 - val_acc: 0.7500\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5006 - acc: 0.7587 - val_loss: 0.5132 - val_acc: 0.7500\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5004 - acc: 0.7569 - val_loss: 0.5131 - val_acc: 0.7500\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5003 - acc: 0.7569 - val_loss: 0.5130 - val_acc: 0.7500\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5001 - acc: 0.7604 - val_loss: 0.5128 - val_acc: 0.7500\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4999 - acc: 0.7569 - val_loss: 0.5127 - val_acc: 0.7500\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4998 - acc: 0.7587 - val_loss: 0.5126 - val_acc: 0.7500\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4996 - acc: 0.7587 - val_loss: 0.5124 - val_acc: 0.7552\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4995 - acc: 0.7587 - val_loss: 0.5123 - val_acc: 0.7552\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4993 - acc: 0.7587 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4992 - acc: 0.7587 - val_loss: 0.5120 - val_acc: 0.7552\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4990 - acc: 0.7587 - val_loss: 0.5119 - val_acc: 0.7552\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4989 - acc: 0.7587 - val_loss: 0.5118 - val_acc: 0.7552\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4987 - acc: 0.7587 - val_loss: 0.5116 - val_acc: 0.7552\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4986 - acc: 0.7587 - val_loss: 0.5115 - val_acc: 0.7552\n",
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4984 - acc: 0.7587 - val_loss: 0.5114 - val_acc: 0.7552\n",
      "Epoch 240/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4982 - acc: 0.7587 - val_loss: 0.5112 - val_acc: 0.7552\n",
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4981 - acc: 0.7587 - val_loss: 0.5111 - val_acc: 0.7552\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4979 - acc: 0.7604 - val_loss: 0.5110 - val_acc: 0.7552\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4978 - acc: 0.7604 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4977 - acc: 0.7604 - val_loss: 0.5107 - val_acc: 0.7552\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4975 - acc: 0.7587 - val_loss: 0.5106 - val_acc: 0.7552\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4974 - acc: 0.7587 - val_loss: 0.5105 - val_acc: 0.7552\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4972 - acc: 0.7587 - val_loss: 0.5104 - val_acc: 0.7552\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4971 - acc: 0.7587 - val_loss: 0.5102 - val_acc: 0.7552\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4969 - acc: 0.7587 - val_loss: 0.5101 - val_acc: 0.7552\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4968 - acc: 0.7587 - val_loss: 0.5100 - val_acc: 0.7552\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4966 - acc: 0.7587 - val_loss: 0.5099 - val_acc: 0.7552\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4965 - acc: 0.7587 - val_loss: 0.5097 - val_acc: 0.7552\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4963 - acc: 0.7587 - val_loss: 0.5096 - val_acc: 0.7552\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4962 - acc: 0.7587 - val_loss: 0.5095 - val_acc: 0.7604\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4961 - acc: 0.7587 - val_loss: 0.5094 - val_acc: 0.7604\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4959 - acc: 0.7604 - val_loss: 0.5092 - val_acc: 0.7604\n",
      "Epoch 257/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4958 - acc: 0.7604 - val_loss: 0.5091 - val_acc: 0.7604\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4956 - acc: 0.7604 - val_loss: 0.5090 - val_acc: 0.7604\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4955 - acc: 0.7604 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4953 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4952 - acc: 0.7604 - val_loss: 0.5087 - val_acc: 0.7656\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4951 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7656\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4949 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4948 - acc: 0.7604 - val_loss: 0.5083 - val_acc: 0.7656\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4946 - acc: 0.7604 - val_loss: 0.5082 - val_acc: 0.7656\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4945 - acc: 0.7622 - val_loss: 0.5081 - val_acc: 0.7656\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4944 - acc: 0.7622 - val_loss: 0.5080 - val_acc: 0.7656\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4942 - acc: 0.7622 - val_loss: 0.5078 - val_acc: 0.7656\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4941 - acc: 0.7622 - val_loss: 0.5077 - val_acc: 0.7656\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4940 - acc: 0.7622 - val_loss: 0.5076 - val_acc: 0.7656\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4938 - acc: 0.7622 - val_loss: 0.5075 - val_acc: 0.7656\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4937 - acc: 0.7622 - val_loss: 0.5074 - val_acc: 0.7656\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4935 - acc: 0.7587 - val_loss: 0.5073 - val_acc: 0.7656\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4934 - acc: 0.7587 - val_loss: 0.5072 - val_acc: 0.7656\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4933 - acc: 0.7587 - val_loss: 0.5070 - val_acc: 0.7604\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4932 - acc: 0.7587 - val_loss: 0.5069 - val_acc: 0.7604\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4930 - acc: 0.7587 - val_loss: 0.5068 - val_acc: 0.7604\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4929 - acc: 0.7587 - val_loss: 0.5067 - val_acc: 0.7604\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4928 - acc: 0.7587 - val_loss: 0.5066 - val_acc: 0.7552\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4926 - acc: 0.7587 - val_loss: 0.5065 - val_acc: 0.7552\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4925 - acc: 0.7587 - val_loss: 0.5064 - val_acc: 0.7552\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4924 - acc: 0.7587 - val_loss: 0.5063 - val_acc: 0.7552\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4922 - acc: 0.7587 - val_loss: 0.5062 - val_acc: 0.7552\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4921 - acc: 0.7587 - val_loss: 0.5061 - val_acc: 0.7552\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4920 - acc: 0.7587 - val_loss: 0.5060 - val_acc: 0.7552\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4918 - acc: 0.7587 - val_loss: 0.5058 - val_acc: 0.7552\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4917 - acc: 0.7587 - val_loss: 0.5057 - val_acc: 0.7552\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4916 - acc: 0.7587 - val_loss: 0.5056 - val_acc: 0.7552\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4915 - acc: 0.7587 - val_loss: 0.5055 - val_acc: 0.7552\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4914 - acc: 0.7604 - val_loss: 0.5054 - val_acc: 0.7552\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4912 - acc: 0.7604 - val_loss: 0.5053 - val_acc: 0.7552\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4911 - acc: 0.7604 - val_loss: 0.5052 - val_acc: 0.7552\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4910 - acc: 0.7604 - val_loss: 0.5051 - val_acc: 0.7552\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4908 - acc: 0.7604 - val_loss: 0.5050 - val_acc: 0.7500\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4907 - acc: 0.7604 - val_loss: 0.5049 - val_acc: 0.7500\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4906 - acc: 0.7604 - val_loss: 0.5048 - val_acc: 0.7500\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4905 - acc: 0.7604 - val_loss: 0.5047 - val_acc: 0.7500\n",
      "Epoch 298/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4903 - acc: 0.7604 - val_loss: 0.5046 - val_acc: 0.7500\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4902 - acc: 0.7604 - val_loss: 0.5045 - val_acc: 0.7500\n",
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4901 - acc: 0.7604 - val_loss: 0.5044 - val_acc: 0.7500\n",
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4900 - acc: 0.7587 - val_loss: 0.5043 - val_acc: 0.7500\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4899 - acc: 0.7604 - val_loss: 0.5042 - val_acc: 0.7500\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4897 - acc: 0.7604 - val_loss: 0.5041 - val_acc: 0.7448\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4896 - acc: 0.7604 - val_loss: 0.5040 - val_acc: 0.7448\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4895 - acc: 0.7604 - val_loss: 0.5039 - val_acc: 0.7448\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4894 - acc: 0.7587 - val_loss: 0.5038 - val_acc: 0.7448\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4893 - acc: 0.7587 - val_loss: 0.5037 - val_acc: 0.7448\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4891 - acc: 0.7604 - val_loss: 0.5036 - val_acc: 0.7448\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4890 - acc: 0.7604 - val_loss: 0.5035 - val_acc: 0.7448\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4889 - acc: 0.7604 - val_loss: 0.5034 - val_acc: 0.7448\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4888 - acc: 0.7604 - val_loss: 0.5033 - val_acc: 0.7448\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4887 - acc: 0.7587 - val_loss: 0.5032 - val_acc: 0.7448\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4886 - acc: 0.7587 - val_loss: 0.5031 - val_acc: 0.7448\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4884 - acc: 0.7587 - val_loss: 0.5030 - val_acc: 0.7448\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4883 - acc: 0.7604 - val_loss: 0.5029 - val_acc: 0.7448\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4882 - acc: 0.7587 - val_loss: 0.5029 - val_acc: 0.7448\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4881 - acc: 0.7604 - val_loss: 0.5028 - val_acc: 0.7448\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4880 - acc: 0.7604 - val_loss: 0.5027 - val_acc: 0.7448\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4879 - acc: 0.7587 - val_loss: 0.5026 - val_acc: 0.7448\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4878 - acc: 0.7587 - val_loss: 0.5025 - val_acc: 0.7448\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4877 - acc: 0.7587 - val_loss: 0.5024 - val_acc: 0.7448\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4875 - acc: 0.7587 - val_loss: 0.5023 - val_acc: 0.7448\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4874 - acc: 0.7604 - val_loss: 0.5022 - val_acc: 0.7448\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4873 - acc: 0.7587 - val_loss: 0.5021 - val_acc: 0.7448\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4872 - acc: 0.7587 - val_loss: 0.5020 - val_acc: 0.7448\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4871 - acc: 0.7587 - val_loss: 0.5019 - val_acc: 0.7448\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4870 - acc: 0.7587 - val_loss: 0.5018 - val_acc: 0.7448\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4869 - acc: 0.7587 - val_loss: 0.5018 - val_acc: 0.7448\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4867 - acc: 0.7604 - val_loss: 0.5017 - val_acc: 0.7448\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4866 - acc: 0.7587 - val_loss: 0.5016 - val_acc: 0.7448\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4865 - acc: 0.7587 - val_loss: 0.5015 - val_acc: 0.7448\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4864 - acc: 0.7587 - val_loss: 0.5014 - val_acc: 0.7448\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4863 - acc: 0.7587 - val_loss: 0.5013 - val_acc: 0.7448\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4862 - acc: 0.7587 - val_loss: 0.5012 - val_acc: 0.7448\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4861 - acc: 0.7569 - val_loss: 0.5011 - val_acc: 0.7396\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4860 - acc: 0.7569 - val_loss: 0.5011 - val_acc: 0.7396\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4859 - acc: 0.7569 - val_loss: 0.5010 - val_acc: 0.7396\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4858 - acc: 0.7569 - val_loss: 0.5009 - val_acc: 0.7396\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4857 - acc: 0.7587 - val_loss: 0.5008 - val_acc: 0.7396\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4855 - acc: 0.7569 - val_loss: 0.5007 - val_acc: 0.7396\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4855 - acc: 0.7587 - val_loss: 0.5006 - val_acc: 0.7396\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4853 - acc: 0.7569 - val_loss: 0.5005 - val_acc: 0.7396\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4852 - acc: 0.7587 - val_loss: 0.5005 - val_acc: 0.7448\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4851 - acc: 0.7587 - val_loss: 0.5004 - val_acc: 0.7448\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4850 - acc: 0.7587 - val_loss: 0.5003 - val_acc: 0.7448\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4849 - acc: 0.7587 - val_loss: 0.5002 - val_acc: 0.7448\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4849 - acc: 0.7587 - val_loss: 0.5001 - val_acc: 0.7448\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4847 - acc: 0.7587 - val_loss: 0.5001 - val_acc: 0.7448\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4846 - acc: 0.7587 - val_loss: 0.5000 - val_acc: 0.7448\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4845 - acc: 0.7587 - val_loss: 0.4999 - val_acc: 0.7448\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4844 - acc: 0.7587 - val_loss: 0.4998 - val_acc: 0.7448\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4844 - acc: 0.7587 - val_loss: 0.4997 - val_acc: 0.7448\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4842 - acc: 0.7569 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4841 - acc: 0.7587 - val_loss: 0.4996 - val_acc: 0.7448\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4840 - acc: 0.7587 - val_loss: 0.4995 - val_acc: 0.7448\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4839 - acc: 0.7587 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 357/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4838 - acc: 0.7587 - val_loss: 0.4993 - val_acc: 0.7448\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4837 - acc: 0.7587 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4836 - acc: 0.7587 - val_loss: 0.4992 - val_acc: 0.7448\n",
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4835 - acc: 0.7569 - val_loss: 0.4991 - val_acc: 0.7448\n",
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4834 - acc: 0.7569 - val_loss: 0.4990 - val_acc: 0.7448\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4833 - acc: 0.7569 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4832 - acc: 0.7569 - val_loss: 0.4989 - val_acc: 0.7448\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4831 - acc: 0.7587 - val_loss: 0.4988 - val_acc: 0.7448\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4830 - acc: 0.7569 - val_loss: 0.4987 - val_acc: 0.7448\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4829 - acc: 0.7587 - val_loss: 0.4986 - val_acc: 0.7448\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4829 - acc: 0.7587 - val_loss: 0.4986 - val_acc: 0.7448\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4828 - acc: 0.7587 - val_loss: 0.4985 - val_acc: 0.7448\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4827 - acc: 0.7569 - val_loss: 0.4984 - val_acc: 0.7448\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4826 - acc: 0.7587 - val_loss: 0.4983 - val_acc: 0.7448\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4825 - acc: 0.7587 - val_loss: 0.4983 - val_acc: 0.7448\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4824 - acc: 0.7587 - val_loss: 0.4982 - val_acc: 0.7448\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4823 - acc: 0.7587 - val_loss: 0.4981 - val_acc: 0.7448\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4822 - acc: 0.7604 - val_loss: 0.4980 - val_acc: 0.7448\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4821 - acc: 0.7604 - val_loss: 0.4980 - val_acc: 0.7448\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4820 - acc: 0.7604 - val_loss: 0.4979 - val_acc: 0.7448\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4819 - acc: 0.7604 - val_loss: 0.4978 - val_acc: 0.7448\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4818 - acc: 0.7604 - val_loss: 0.4977 - val_acc: 0.7448\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4817 - acc: 0.7604 - val_loss: 0.4977 - val_acc: 0.7448\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4816 - acc: 0.7604 - val_loss: 0.4976 - val_acc: 0.7448\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4815 - acc: 0.7604 - val_loss: 0.4975 - val_acc: 0.7448\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4815 - acc: 0.7604 - val_loss: 0.4975 - val_acc: 0.7448\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4814 - acc: 0.7604 - val_loss: 0.4974 - val_acc: 0.7448\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4813 - acc: 0.7622 - val_loss: 0.4973 - val_acc: 0.7448\n",
      "Epoch 385/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4812 - acc: 0.7622 - val_loss: 0.4973 - val_acc: 0.7448\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4811 - acc: 0.7622 - val_loss: 0.4972 - val_acc: 0.7448\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4810 - acc: 0.7622 - val_loss: 0.4971 - val_acc: 0.7448\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4809 - acc: 0.7622 - val_loss: 0.4970 - val_acc: 0.7448\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4808 - acc: 0.7622 - val_loss: 0.4970 - val_acc: 0.7448\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4807 - acc: 0.7622 - val_loss: 0.4969 - val_acc: 0.7448\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4807 - acc: 0.7622 - val_loss: 0.4968 - val_acc: 0.7448\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4806 - acc: 0.7622 - val_loss: 0.4968 - val_acc: 0.7448\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4805 - acc: 0.7622 - val_loss: 0.4967 - val_acc: 0.7448\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4804 - acc: 0.7622 - val_loss: 0.4966 - val_acc: 0.7448\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4803 - acc: 0.7622 - val_loss: 0.4966 - val_acc: 0.7448\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4802 - acc: 0.7622 - val_loss: 0.4965 - val_acc: 0.7448\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4801 - acc: 0.7622 - val_loss: 0.4964 - val_acc: 0.7448\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4800 - acc: 0.7622 - val_loss: 0.4964 - val_acc: 0.7448\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4800 - acc: 0.7639 - val_loss: 0.4963 - val_acc: 0.7448\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4799 - acc: 0.7656 - val_loss: 0.4962 - val_acc: 0.7448\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4798 - acc: 0.7639 - val_loss: 0.4962 - val_acc: 0.7448\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4797 - acc: 0.7639 - val_loss: 0.4961 - val_acc: 0.7448\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4796 - acc: 0.7639 - val_loss: 0.4960 - val_acc: 0.7448\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4795 - acc: 0.7639 - val_loss: 0.4960 - val_acc: 0.7448\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4794 - acc: 0.7639 - val_loss: 0.4959 - val_acc: 0.7448\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4794 - acc: 0.7639 - val_loss: 0.4958 - val_acc: 0.7448\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4793 - acc: 0.7639 - val_loss: 0.4958 - val_acc: 0.7448\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4792 - acc: 0.7639 - val_loss: 0.4957 - val_acc: 0.7448\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4791 - acc: 0.7639 - val_loss: 0.4957 - val_acc: 0.7448\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4790 - acc: 0.7639 - val_loss: 0.4956 - val_acc: 0.7448\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4790 - acc: 0.7639 - val_loss: 0.4955 - val_acc: 0.7448\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.4789 - acc: 0.7639 - val_loss: 0.4955 - val_acc: 0.7448\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.4788 - acc: 0.7639 - val_loss: 0.4954 - val_acc: 0.7448\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.4787 - acc: 0.7639 - val_loss: 0.4953 - val_acc: 0.7448\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s 136us/step - loss: 0.4786 - acc: 0.7656 - val_loss: 0.4953 - val_acc: 0.7448\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s 104us/step - loss: 0.4786 - acc: 0.7639 - val_loss: 0.4952 - val_acc: 0.7448\n",
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.4785 - acc: 0.7639 - val_loss: 0.4952 - val_acc: 0.7448\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s 136us/step - loss: 0.4784 - acc: 0.7639 - val_loss: 0.4951 - val_acc: 0.7448\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.4783 - acc: 0.7639 - val_loss: 0.4950 - val_acc: 0.7448\n",
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.4782 - acc: 0.7639 - val_loss: 0.4950 - val_acc: 0.7448\n",
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.4782 - acc: 0.7639 - val_loss: 0.4949 - val_acc: 0.7448\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s 133us/step - loss: 0.4781 - acc: 0.7656 - val_loss: 0.4949 - val_acc: 0.7448\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s 206us/step - loss: 0.4780 - acc: 0.7656 - val_loss: 0.4948 - val_acc: 0.7448\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4779 - acc: 0.7656 - val_loss: 0.4947 - val_acc: 0.7448\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s 123us/step - loss: 0.4778 - acc: 0.7656 - val_loss: 0.4947 - val_acc: 0.7448\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s 143us/step - loss: 0.4778 - acc: 0.7656 - val_loss: 0.4946 - val_acc: 0.7448\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s 165us/step - loss: 0.4777 - acc: 0.7656 - val_loss: 0.4946 - val_acc: 0.7448\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4776 - acc: 0.7656 - val_loss: 0.4945 - val_acc: 0.7448\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s 125us/step - loss: 0.4775 - acc: 0.7656 - val_loss: 0.4944 - val_acc: 0.7448\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s 108us/step - loss: 0.4775 - acc: 0.7656 - val_loss: 0.4944 - val_acc: 0.7448\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s 136us/step - loss: 0.4774 - acc: 0.7656 - val_loss: 0.4943 - val_acc: 0.7448\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s 133us/step - loss: 0.4773 - acc: 0.7656 - val_loss: 0.4943 - val_acc: 0.7448\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.4772 - acc: 0.7656 - val_loss: 0.4942 - val_acc: 0.7448\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s 136us/step - loss: 0.4771 - acc: 0.7656 - val_loss: 0.4942 - val_acc: 0.7448\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s 104us/step - loss: 0.4771 - acc: 0.7656 - val_loss: 0.4941 - val_acc: 0.7448\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.4770 - acc: 0.7656 - val_loss: 0.4940 - val_acc: 0.7448\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s 164us/step - loss: 0.4769 - acc: 0.7656 - val_loss: 0.4940 - val_acc: 0.7448\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s 137us/step - loss: 0.4769 - acc: 0.7674 - val_loss: 0.4939 - val_acc: 0.7448\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4768 - acc: 0.7674 - val_loss: 0.4939 - val_acc: 0.7448\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.4767 - acc: 0.7656 - val_loss: 0.4938 - val_acc: 0.7448\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s 136us/step - loss: 0.4766 - acc: 0.7656 - val_loss: 0.4938 - val_acc: 0.7448\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.4766 - acc: 0.7674 - val_loss: 0.4937 - val_acc: 0.7448\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4765 - acc: 0.7656 - val_loss: 0.4937 - val_acc: 0.7448\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4764 - acc: 0.7674 - val_loss: 0.4936 - val_acc: 0.7448\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4764 - acc: 0.7674 - val_loss: 0.4935 - val_acc: 0.7448\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4763 - acc: 0.7656 - val_loss: 0.4935 - val_acc: 0.7448\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4762 - acc: 0.7674 - val_loss: 0.4934 - val_acc: 0.7448\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4761 - acc: 0.7674 - val_loss: 0.4934 - val_acc: 0.7448\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4760 - acc: 0.7656 - val_loss: 0.4933 - val_acc: 0.7448\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4760 - acc: 0.7656 - val_loss: 0.4933 - val_acc: 0.7448\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4759 - acc: 0.7656 - val_loss: 0.4932 - val_acc: 0.7448\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4758 - acc: 0.7674 - val_loss: 0.4932 - val_acc: 0.7448\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4758 - acc: 0.7656 - val_loss: 0.4931 - val_acc: 0.7448\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4757 - acc: 0.7674 - val_loss: 0.4931 - val_acc: 0.7448\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4756 - acc: 0.7674 - val_loss: 0.4930 - val_acc: 0.7448\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4756 - acc: 0.7674 - val_loss: 0.4930 - val_acc: 0.7448\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4755 - acc: 0.7674 - val_loss: 0.4929 - val_acc: 0.7448\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4754 - acc: 0.7674 - val_loss: 0.4929 - val_acc: 0.7448\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4753 - acc: 0.7656 - val_loss: 0.4928 - val_acc: 0.7448\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4753 - acc: 0.7674 - val_loss: 0.4928 - val_acc: 0.7448\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4752 - acc: 0.7674 - val_loss: 0.4927 - val_acc: 0.7448\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4751 - acc: 0.7674 - val_loss: 0.4927 - val_acc: 0.7448\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4751 - acc: 0.7674 - val_loss: 0.4926 - val_acc: 0.7448\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4750 - acc: 0.7674 - val_loss: 0.4926 - val_acc: 0.7448\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4749 - acc: 0.7691 - val_loss: 0.4925 - val_acc: 0.7448\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4749 - acc: 0.7674 - val_loss: 0.4925 - val_acc: 0.7448\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4748 - acc: 0.7691 - val_loss: 0.4924 - val_acc: 0.7448\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4747 - acc: 0.7691 - val_loss: 0.4924 - val_acc: 0.7448\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4747 - acc: 0.7691 - val_loss: 0.4923 - val_acc: 0.7448\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4746 - acc: 0.7691 - val_loss: 0.4923 - val_acc: 0.7448\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4745 - acc: 0.7674 - val_loss: 0.4922 - val_acc: 0.7448\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4745 - acc: 0.7691 - val_loss: 0.4922 - val_acc: 0.7448\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4744 - acc: 0.7691 - val_loss: 0.4921 - val_acc: 0.7448\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4743 - acc: 0.7691 - val_loss: 0.4921 - val_acc: 0.7448\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4743 - acc: 0.7691 - val_loss: 0.4920 - val_acc: 0.7448\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4742 - acc: 0.7691 - val_loss: 0.4920 - val_acc: 0.7448\n",
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4741 - acc: 0.7691 - val_loss: 0.4919 - val_acc: 0.7448\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4741 - acc: 0.7691 - val_loss: 0.4919 - val_acc: 0.7448\n",
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4740 - acc: 0.7674 - val_loss: 0.4918 - val_acc: 0.7396\n",
      "Epoch 480/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4739 - acc: 0.7674 - val_loss: 0.4918 - val_acc: 0.7396\n",
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4739 - acc: 0.7656 - val_loss: 0.4917 - val_acc: 0.7396\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4738 - acc: 0.7639 - val_loss: 0.4917 - val_acc: 0.7396\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4738 - acc: 0.7656 - val_loss: 0.4916 - val_acc: 0.7396\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4737 - acc: 0.7639 - val_loss: 0.4916 - val_acc: 0.7396\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4736 - acc: 0.7639 - val_loss: 0.4916 - val_acc: 0.7396\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4736 - acc: 0.7639 - val_loss: 0.4915 - val_acc: 0.7396\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4735 - acc: 0.7639 - val_loss: 0.4915 - val_acc: 0.7396\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4734 - acc: 0.7639 - val_loss: 0.4914 - val_acc: 0.7396\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4734 - acc: 0.7639 - val_loss: 0.4914 - val_acc: 0.7396\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4733 - acc: 0.7639 - val_loss: 0.4913 - val_acc: 0.7396\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4732 - acc: 0.7639 - val_loss: 0.4913 - val_acc: 0.7396\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4732 - acc: 0.7639 - val_loss: 0.4912 - val_acc: 0.7396\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4731 - acc: 0.7639 - val_loss: 0.4912 - val_acc: 0.7396\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4731 - acc: 0.7639 - val_loss: 0.4911 - val_acc: 0.7396\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4730 - acc: 0.7639 - val_loss: 0.4911 - val_acc: 0.7396\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4730 - acc: 0.7639 - val_loss: 0.4911 - val_acc: 0.7396\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4729 - acc: 0.7639 - val_loss: 0.4910 - val_acc: 0.7396\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4728 - acc: 0.7639 - val_loss: 0.4910 - val_acc: 0.7396\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4728 - acc: 0.7639 - val_loss: 0.4909 - val_acc: 0.7396\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4727 - acc: 0.7639 - val_loss: 0.4909 - val_acc: 0.7396\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4726 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7396\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4726 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7396\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4725 - acc: 0.7639 - val_loss: 0.4908 - val_acc: 0.7396\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4725 - acc: 0.7639 - val_loss: 0.4907 - val_acc: 0.7396\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4724 - acc: 0.7639 - val_loss: 0.4907 - val_acc: 0.7396\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4723 - acc: 0.7639 - val_loss: 0.4906 - val_acc: 0.7396\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4723 - acc: 0.7639 - val_loss: 0.4906 - val_acc: 0.7396\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4722 - acc: 0.7639 - val_loss: 0.4905 - val_acc: 0.7396\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4722 - acc: 0.7639 - val_loss: 0.4905 - val_acc: 0.7396\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4721 - acc: 0.7639 - val_loss: 0.4905 - val_acc: 0.7396\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4720 - acc: 0.7639 - val_loss: 0.4904 - val_acc: 0.7396\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4720 - acc: 0.7656 - val_loss: 0.4904 - val_acc: 0.7396\n",
      "Epoch 513/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4719 - acc: 0.7656 - val_loss: 0.4903 - val_acc: 0.7396\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4719 - acc: 0.7656 - val_loss: 0.4903 - val_acc: 0.7396\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4718 - acc: 0.7639 - val_loss: 0.4903 - val_acc: 0.7396\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4718 - acc: 0.7656 - val_loss: 0.4902 - val_acc: 0.7396\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4717 - acc: 0.7656 - val_loss: 0.4902 - val_acc: 0.7396\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4717 - acc: 0.7656 - val_loss: 0.4901 - val_acc: 0.7396\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4716 - acc: 0.7656 - val_loss: 0.4901 - val_acc: 0.7396\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4715 - acc: 0.7656 - val_loss: 0.4901 - val_acc: 0.7396\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4715 - acc: 0.7656 - val_loss: 0.4900 - val_acc: 0.7396\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4714 - acc: 0.7656 - val_loss: 0.4900 - val_acc: 0.7396\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4714 - acc: 0.7656 - val_loss: 0.4899 - val_acc: 0.7396\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4713 - acc: 0.7656 - val_loss: 0.4899 - val_acc: 0.7396\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4713 - acc: 0.7656 - val_loss: 0.4899 - val_acc: 0.7396\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4712 - acc: 0.7656 - val_loss: 0.4898 - val_acc: 0.7396\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4712 - acc: 0.7656 - val_loss: 0.4898 - val_acc: 0.7396\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4711 - acc: 0.7674 - val_loss: 0.4898 - val_acc: 0.7396\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4710 - acc: 0.7656 - val_loss: 0.4897 - val_acc: 0.7396\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4710 - acc: 0.7674 - val_loss: 0.4897 - val_acc: 0.7396\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4709 - acc: 0.7674 - val_loss: 0.4896 - val_acc: 0.7396\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4709 - acc: 0.7674 - val_loss: 0.4896 - val_acc: 0.7396\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4708 - acc: 0.7674 - val_loss: 0.4896 - val_acc: 0.7396\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4708 - acc: 0.7674 - val_loss: 0.4895 - val_acc: 0.7396\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4707 - acc: 0.7674 - val_loss: 0.4895 - val_acc: 0.7396\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4707 - acc: 0.7674 - val_loss: 0.4894 - val_acc: 0.7396\n",
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4706 - acc: 0.7674 - val_loss: 0.4894 - val_acc: 0.7396\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4706 - acc: 0.7674 - val_loss: 0.4894 - val_acc: 0.7396\n",
      "Epoch 539/1000\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.4705 - acc: 0.7674 - val_loss: 0.4893 - val_acc: 0.7396\n",
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4705 - acc: 0.7674 - val_loss: 0.4893 - val_acc: 0.7396\n",
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4704 - acc: 0.7674 - val_loss: 0.4893 - val_acc: 0.7396\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4704 - acc: 0.7674 - val_loss: 0.4892 - val_acc: 0.7396\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4703 - acc: 0.7674 - val_loss: 0.4892 - val_acc: 0.7396\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4703 - acc: 0.7674 - val_loss: 0.4892 - val_acc: 0.7396\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4702 - acc: 0.7674 - val_loss: 0.4891 - val_acc: 0.7396\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4702 - acc: 0.7674 - val_loss: 0.4891 - val_acc: 0.7396\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4701 - acc: 0.7691 - val_loss: 0.4891 - val_acc: 0.7396\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4701 - acc: 0.7674 - val_loss: 0.4890 - val_acc: 0.7396\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4700 - acc: 0.7691 - val_loss: 0.4890 - val_acc: 0.7396\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4700 - acc: 0.7691 - val_loss: 0.4889 - val_acc: 0.7396\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4699 - acc: 0.7691 - val_loss: 0.4889 - val_acc: 0.7396\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4698 - acc: 0.7691 - val_loss: 0.4889 - val_acc: 0.7396\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4698 - acc: 0.7691 - val_loss: 0.4888 - val_acc: 0.7396\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4697 - acc: 0.7691 - val_loss: 0.4888 - val_acc: 0.7396\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4697 - acc: 0.7691 - val_loss: 0.4888 - val_acc: 0.7396\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4696 - acc: 0.7691 - val_loss: 0.4887 - val_acc: 0.7396\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4696 - acc: 0.7691 - val_loss: 0.4887 - val_acc: 0.7396\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4695 - acc: 0.7691 - val_loss: 0.4887 - val_acc: 0.7396\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4695 - acc: 0.7691 - val_loss: 0.4886 - val_acc: 0.7396\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4694 - acc: 0.7691 - val_loss: 0.4886 - val_acc: 0.7396\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4694 - acc: 0.7691 - val_loss: 0.4886 - val_acc: 0.7396\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4694 - acc: 0.7691 - val_loss: 0.4885 - val_acc: 0.7396\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4693 - acc: 0.7691 - val_loss: 0.4885 - val_acc: 0.7396\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4693 - acc: 0.7691 - val_loss: 0.4885 - val_acc: 0.7396\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4692 - acc: 0.7691 - val_loss: 0.4884 - val_acc: 0.7396\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4692 - acc: 0.7691 - val_loss: 0.4884 - val_acc: 0.7396\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4691 - acc: 0.7691 - val_loss: 0.4884 - val_acc: 0.7396\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4691 - acc: 0.7691 - val_loss: 0.4883 - val_acc: 0.7396\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4690 - acc: 0.7691 - val_loss: 0.4883 - val_acc: 0.7396\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4690 - acc: 0.7691 - val_loss: 0.4883 - val_acc: 0.7396\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4689 - acc: 0.7691 - val_loss: 0.4883 - val_acc: 0.7396\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4689 - acc: 0.7674 - val_loss: 0.4882 - val_acc: 0.7396\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4688 - acc: 0.7674 - val_loss: 0.4882 - val_acc: 0.7396\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4688 - acc: 0.7674 - val_loss: 0.4882 - val_acc: 0.7396\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4688 - acc: 0.7691 - val_loss: 0.4881 - val_acc: 0.7396\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4687 - acc: 0.7674 - val_loss: 0.4881 - val_acc: 0.7396\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4687 - acc: 0.7691 - val_loss: 0.4881 - val_acc: 0.7396\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4686 - acc: 0.7674 - val_loss: 0.4880 - val_acc: 0.7396\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4686 - acc: 0.7691 - val_loss: 0.4880 - val_acc: 0.7396\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4685 - acc: 0.7691 - val_loss: 0.4880 - val_acc: 0.7396\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4685 - acc: 0.7691 - val_loss: 0.4879 - val_acc: 0.7396\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4684 - acc: 0.7691 - val_loss: 0.4879 - val_acc: 0.7396\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4684 - acc: 0.7674 - val_loss: 0.4879 - val_acc: 0.7396\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4683 - acc: 0.7691 - val_loss: 0.4879 - val_acc: 0.7396\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4683 - acc: 0.7691 - val_loss: 0.4878 - val_acc: 0.7396\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4682 - acc: 0.7674 - val_loss: 0.4878 - val_acc: 0.7396\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4682 - acc: 0.7674 - val_loss: 0.4878 - val_acc: 0.7396\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4682 - acc: 0.7674 - val_loss: 0.4877 - val_acc: 0.7396\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4681 - acc: 0.7674 - val_loss: 0.4877 - val_acc: 0.7396\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4681 - acc: 0.7674 - val_loss: 0.4877 - val_acc: 0.7396\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4680 - acc: 0.7674 - val_loss: 0.4876 - val_acc: 0.7396\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4680 - acc: 0.7674 - val_loss: 0.4876 - val_acc: 0.7396\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4679 - acc: 0.7674 - val_loss: 0.4876 - val_acc: 0.7396\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4679 - acc: 0.7674 - val_loss: 0.4876 - val_acc: 0.7396\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4679 - acc: 0.7674 - val_loss: 0.4875 - val_acc: 0.7396\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4678 - acc: 0.7674 - val_loss: 0.4875 - val_acc: 0.7396\n",
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4678 - acc: 0.7691 - val_loss: 0.4875 - val_acc: 0.7396\n",
      "Epoch 598/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4677 - acc: 0.7691 - val_loss: 0.4874 - val_acc: 0.7396\n",
      "Epoch 599/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4677 - acc: 0.7691 - val_loss: 0.4874 - val_acc: 0.7396\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4676 - acc: 0.7691 - val_loss: 0.4874 - val_acc: 0.7396\n",
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4676 - acc: 0.7691 - val_loss: 0.4874 - val_acc: 0.7396\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4675 - acc: 0.7691 - val_loss: 0.4873 - val_acc: 0.7396\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4675 - acc: 0.7691 - val_loss: 0.4873 - val_acc: 0.7396\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4675 - acc: 0.7691 - val_loss: 0.4873 - val_acc: 0.7396\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4674 - acc: 0.7691 - val_loss: 0.4873 - val_acc: 0.7396\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4674 - acc: 0.7691 - val_loss: 0.4872 - val_acc: 0.7396\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4673 - acc: 0.7691 - val_loss: 0.4872 - val_acc: 0.7396\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4673 - acc: 0.7691 - val_loss: 0.4872 - val_acc: 0.7396\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4672 - acc: 0.7691 - val_loss: 0.4871 - val_acc: 0.7396\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4672 - acc: 0.7691 - val_loss: 0.4871 - val_acc: 0.7396\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4672 - acc: 0.7691 - val_loss: 0.4871 - val_acc: 0.7396\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4671 - acc: 0.7691 - val_loss: 0.4871 - val_acc: 0.7396\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4671 - acc: 0.7691 - val_loss: 0.4870 - val_acc: 0.7396\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4670 - acc: 0.7691 - val_loss: 0.4870 - val_acc: 0.7396\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4670 - acc: 0.7691 - val_loss: 0.4870 - val_acc: 0.7396\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4670 - acc: 0.7691 - val_loss: 0.4870 - val_acc: 0.7396\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4669 - acc: 0.7691 - val_loss: 0.4869 - val_acc: 0.7396\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4669 - acc: 0.7691 - val_loss: 0.4869 - val_acc: 0.7396\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4668 - acc: 0.7691 - val_loss: 0.4869 - val_acc: 0.7396\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4668 - acc: 0.7691 - val_loss: 0.4869 - val_acc: 0.7396\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4668 - acc: 0.7691 - val_loss: 0.4868 - val_acc: 0.7396\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4667 - acc: 0.7691 - val_loss: 0.4868 - val_acc: 0.7396\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4667 - acc: 0.7691 - val_loss: 0.4868 - val_acc: 0.7396\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4667 - acc: 0.7691 - val_loss: 0.4868 - val_acc: 0.7396\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4666 - acc: 0.7691 - val_loss: 0.4867 - val_acc: 0.7448\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4666 - acc: 0.7691 - val_loss: 0.4867 - val_acc: 0.7448\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4665 - acc: 0.7691 - val_loss: 0.4867 - val_acc: 0.7448\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4665 - acc: 0.7691 - val_loss: 0.4867 - val_acc: 0.7448\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4665 - acc: 0.7691 - val_loss: 0.4866 - val_acc: 0.7448\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4664 - acc: 0.7691 - val_loss: 0.4866 - val_acc: 0.7448\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4664 - acc: 0.7691 - val_loss: 0.4866 - val_acc: 0.7448\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4663 - acc: 0.7691 - val_loss: 0.4866 - val_acc: 0.7448\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4663 - acc: 0.7691 - val_loss: 0.4865 - val_acc: 0.7448\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4663 - acc: 0.7691 - val_loss: 0.4865 - val_acc: 0.7448\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4662 - acc: 0.7691 - val_loss: 0.4865 - val_acc: 0.7448\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4662 - acc: 0.7691 - val_loss: 0.4865 - val_acc: 0.7448\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4662 - acc: 0.7691 - val_loss: 0.4864 - val_acc: 0.7448\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4661 - acc: 0.7691 - val_loss: 0.4864 - val_acc: 0.7448\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4661 - acc: 0.7691 - val_loss: 0.4864 - val_acc: 0.7448\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4660 - acc: 0.7691 - val_loss: 0.4864 - val_acc: 0.7448\n",
      "Epoch 641/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4660 - acc: 0.7691 - val_loss: 0.4863 - val_acc: 0.7448\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4660 - acc: 0.7691 - val_loss: 0.4863 - val_acc: 0.7448\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4659 - acc: 0.7691 - val_loss: 0.4863 - val_acc: 0.7448\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4659 - acc: 0.7691 - val_loss: 0.4863 - val_acc: 0.7448\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4658 - acc: 0.7691 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4658 - acc: 0.7691 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4658 - acc: 0.7691 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4658 - acc: 0.7691 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4657 - acc: 0.7691 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4657 - acc: 0.7691 - val_loss: 0.4861 - val_acc: 0.7448\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4656 - acc: 0.7691 - val_loss: 0.4861 - val_acc: 0.7448\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4656 - acc: 0.7691 - val_loss: 0.4861 - val_acc: 0.7448\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4656 - acc: 0.7691 - val_loss: 0.4861 - val_acc: 0.7448\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4655 - acc: 0.7691 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4655 - acc: 0.7708 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4655 - acc: 0.7708 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 657/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4654 - acc: 0.7691 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 658/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4654 - acc: 0.7691 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4654 - acc: 0.7691 - val_loss: 0.4859 - val_acc: 0.7448\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4653 - acc: 0.7708 - val_loss: 0.4859 - val_acc: 0.7448\n",
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4653 - acc: 0.7708 - val_loss: 0.4859 - val_acc: 0.7448\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4652 - acc: 0.7708 - val_loss: 0.4859 - val_acc: 0.7448\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4652 - acc: 0.7708 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4652 - acc: 0.7708 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4652 - acc: 0.7708 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4651 - acc: 0.7708 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4651 - acc: 0.7708 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4651 - acc: 0.7708 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4650 - acc: 0.7708 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4650 - acc: 0.7708 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4649 - acc: 0.7708 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4649 - acc: 0.7708 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4649 - acc: 0.7708 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4648 - acc: 0.7708 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4648 - acc: 0.7708 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4648 - acc: 0.7708 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4647 - acc: 0.7708 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4647 - acc: 0.7708 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4647 - acc: 0.7708 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4646 - acc: 0.7708 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4646 - acc: 0.7708 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4646 - acc: 0.7708 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4645 - acc: 0.7708 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4645 - acc: 0.7708 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4645 - acc: 0.7708 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4644 - acc: 0.7708 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4644 - acc: 0.7708 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4644 - acc: 0.7708 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4643 - acc: 0.7708 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4643 - acc: 0.7708 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4643 - acc: 0.7708 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4642 - acc: 0.7708 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4642 - acc: 0.7708 - val_loss: 0.4852 - val_acc: 0.7448\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4642 - acc: 0.7708 - val_loss: 0.4852 - val_acc: 0.7448\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4642 - acc: 0.7708 - val_loss: 0.4852 - val_acc: 0.7500\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4641 - acc: 0.7708 - val_loss: 0.4852 - val_acc: 0.7500\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4641 - acc: 0.7708 - val_loss: 0.4852 - val_acc: 0.7500\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4641 - acc: 0.7708 - val_loss: 0.4852 - val_acc: 0.7500\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4640 - acc: 0.7708 - val_loss: 0.4851 - val_acc: 0.7500\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4640 - acc: 0.7708 - val_loss: 0.4851 - val_acc: 0.7500\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4640 - acc: 0.7708 - val_loss: 0.4851 - val_acc: 0.7500\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4639 - acc: 0.7708 - val_loss: 0.4851 - val_acc: 0.7500\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4639 - acc: 0.7708 - val_loss: 0.4851 - val_acc: 0.7500\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4639 - acc: 0.7708 - val_loss: 0.4850 - val_acc: 0.7500\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4639 - acc: 0.7708 - val_loss: 0.4850 - val_acc: 0.7500\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4638 - acc: 0.7708 - val_loss: 0.4850 - val_acc: 0.7500\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4638 - acc: 0.7708 - val_loss: 0.4850 - val_acc: 0.7500\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4638 - acc: 0.7708 - val_loss: 0.4850 - val_acc: 0.7500\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4637 - acc: 0.7708 - val_loss: 0.4850 - val_acc: 0.7500\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4637 - acc: 0.7708 - val_loss: 0.4849 - val_acc: 0.7500\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4637 - acc: 0.7708 - val_loss: 0.4849 - val_acc: 0.7500\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4636 - acc: 0.7708 - val_loss: 0.4849 - val_acc: 0.7500\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4636 - acc: 0.7708 - val_loss: 0.4849 - val_acc: 0.7500\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4636 - acc: 0.7708 - val_loss: 0.4849 - val_acc: 0.7500\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4636 - acc: 0.7726 - val_loss: 0.4849 - val_acc: 0.7500\n",
      "Epoch 716/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4635 - acc: 0.7726 - val_loss: 0.4848 - val_acc: 0.7500\n",
      "Epoch 717/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4635 - acc: 0.7726 - val_loss: 0.4848 - val_acc: 0.7500\n",
      "Epoch 718/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4635 - acc: 0.7726 - val_loss: 0.4848 - val_acc: 0.7500\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4634 - acc: 0.7726 - val_loss: 0.4848 - val_acc: 0.7500\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4634 - acc: 0.7726 - val_loss: 0.4848 - val_acc: 0.7500\n",
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4634 - acc: 0.7726 - val_loss: 0.4848 - val_acc: 0.7500\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4633 - acc: 0.7726 - val_loss: 0.4847 - val_acc: 0.7500\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4633 - acc: 0.7726 - val_loss: 0.4847 - val_acc: 0.7500\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4633 - acc: 0.7726 - val_loss: 0.4847 - val_acc: 0.7500\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4633 - acc: 0.7726 - val_loss: 0.4847 - val_acc: 0.7500\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4632 - acc: 0.7743 - val_loss: 0.4847 - val_acc: 0.7500\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4632 - acc: 0.7743 - val_loss: 0.4847 - val_acc: 0.7500\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4632 - acc: 0.7743 - val_loss: 0.4846 - val_acc: 0.7500\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4631 - acc: 0.7743 - val_loss: 0.4846 - val_acc: 0.7500\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4631 - acc: 0.7743 - val_loss: 0.4846 - val_acc: 0.7500\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4631 - acc: 0.7743 - val_loss: 0.4846 - val_acc: 0.7500\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4631 - acc: 0.7743 - val_loss: 0.4846 - val_acc: 0.7500\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4630 - acc: 0.7743 - val_loss: 0.4846 - val_acc: 0.7500\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4630 - acc: 0.7726 - val_loss: 0.4845 - val_acc: 0.7552\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4630 - acc: 0.7726 - val_loss: 0.4845 - val_acc: 0.7552\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4630 - acc: 0.7743 - val_loss: 0.4845 - val_acc: 0.7552\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4629 - acc: 0.7743 - val_loss: 0.4845 - val_acc: 0.7552\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4629 - acc: 0.7726 - val_loss: 0.4845 - val_acc: 0.7552\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4629 - acc: 0.7726 - val_loss: 0.4845 - val_acc: 0.7552\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4629 - acc: 0.7726 - val_loss: 0.4845 - val_acc: 0.7552\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4628 - acc: 0.7726 - val_loss: 0.4844 - val_acc: 0.7552\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4628 - acc: 0.7743 - val_loss: 0.4844 - val_acc: 0.7552\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4628 - acc: 0.7726 - val_loss: 0.4844 - val_acc: 0.7552\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4627 - acc: 0.7726 - val_loss: 0.4844 - val_acc: 0.7552\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4628 - acc: 0.7743 - val_loss: 0.4844 - val_acc: 0.7552\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4627 - acc: 0.7726 - val_loss: 0.4844 - val_acc: 0.7552\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4627 - acc: 0.7726 - val_loss: 0.4844 - val_acc: 0.7552\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4626 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7552\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4626 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7552\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4626 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7552\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4626 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7552\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4625 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7552\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4625 - acc: 0.7726 - val_loss: 0.4843 - val_acc: 0.7552\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4625 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7500\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4625 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7500\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4624 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7500\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4624 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7500\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4624 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7500\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4623 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7500\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4623 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7500\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4623 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7500\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4623 - acc: 0.7743 - val_loss: 0.4841 - val_acc: 0.7500\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4622 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7500\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4622 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7500\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4622 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7500\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4622 - acc: 0.7743 - val_loss: 0.4841 - val_acc: 0.7500\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4621 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7500\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4621 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7500\n",
      "Epoch 769/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4621 - acc: 0.7743 - val_loss: 0.4840 - val_acc: 0.7500\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4621 - acc: 0.7726 - val_loss: 0.4840 - val_acc: 0.7500\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4620 - acc: 0.7726 - val_loss: 0.4840 - val_acc: 0.7500\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4840 - val_acc: 0.7500\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4840 - val_acc: 0.7552\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4840 - val_acc: 0.7552\n",
      "Epoch 775/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4620 - acc: 0.7726 - val_loss: 0.4840 - val_acc: 0.7552\n",
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7552\n",
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7552\n",
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7552\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7552\n",
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7552\n",
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7552\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7552\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7552\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7552\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7552\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7552\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7552\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7552\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7552\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7552\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7552\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7552\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7552\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7552\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7552\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7552\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7552\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7552\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7552\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7552\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7552\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4609 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4609 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4609 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4609 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4608 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4608 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4608 - acc: 0.7760 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4608 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7552\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4608 - acc: 0.7743 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4607 - acc: 0.7743 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4607 - acc: 0.7760 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4607 - acc: 0.7760 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4607 - acc: 0.7760 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4606 - acc: 0.7760 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 834/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4606 - acc: 0.7760 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4606 - acc: 0.7760 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4606 - acc: 0.7743 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 837/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4606 - acc: 0.7760 - val_loss: 0.4833 - val_acc: 0.7552\n",
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4605 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4605 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4605 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4605 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4605 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4605 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4604 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4604 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4604 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4604 - acc: 0.7760 - val_loss: 0.4832 - val_acc: 0.7552\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4603 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4603 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4603 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4603 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4603 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4602 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4602 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4602 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4602 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4602 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4602 - acc: 0.7760 - val_loss: 0.4831 - val_acc: 0.7552\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4601 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7552\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4601 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4601 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4601 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4601 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4600 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4600 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4600 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4600 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4599 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4599 - acc: 0.7760 - val_loss: 0.4830 - val_acc: 0.7604\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4599 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4599 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4599 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4599 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4597 - acc: 0.7778 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4597 - acc: 0.7760 - val_loss: 0.4829 - val_acc: 0.7604\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4597 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4597 - acc: 0.7778 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4595 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7604\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4595 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7552\n",
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4595 - acc: 0.7760 - val_loss: 0.4828 - val_acc: 0.7552\n",
      "Epoch 895/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4595 - acc: 0.7760 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4594 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 897/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4594 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4594 - acc: 0.7760 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4594 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4594 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4594 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4594 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4593 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4593 - acc: 0.7760 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4593 - acc: 0.7760 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4593 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4593 - acc: 0.7760 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4593 - acc: 0.7778 - val_loss: 0.4827 - val_acc: 0.7552\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4592 - acc: 0.7778 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4592 - acc: 0.7778 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4592 - acc: 0.7778 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4592 - acc: 0.7760 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4592 - acc: 0.7778 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4591 - acc: 0.7778 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4591 - acc: 0.7778 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4591 - acc: 0.7778 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4591 - acc: 0.7760 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4591 - acc: 0.7760 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4591 - acc: 0.7743 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4590 - acc: 0.7760 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4590 - acc: 0.7760 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4590 - acc: 0.7760 - val_loss: 0.4826 - val_acc: 0.7552\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4590 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7552\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4590 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7552\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4590 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7552\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4589 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7552\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4589 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7552\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4589 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7552\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4589 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7552\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4589 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7500\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4589 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7500\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4589 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7500\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7500\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7500\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7500\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7500\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.4825 - val_acc: 0.7500\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4587 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4587 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4587 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4587 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4587 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4587 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 953/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4585 - acc: 0.7760 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4585 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 955/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4585 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4585 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4585 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4585 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4584 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4585 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4584 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4584 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4584 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4584 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4584 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4583 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4583 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4583 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4583 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4583 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4583 - acc: 0.7760 - val_loss: 0.4823 - val_acc: 0.7500\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4583 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4582 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4582 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4582 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4582 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4582 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4582 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4582 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4581 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4581 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4581 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4581 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4581 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7500\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4581 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7552\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4581 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7552\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4580 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7552\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4580 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7552\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4580 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7552\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4580 - acc: 0.7760 - val_loss: 0.4822 - val_acc: 0.7552\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4580 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4580 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4579 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4579 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4579 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4579 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4579 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4579 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4579 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4578 - acc: 0.7760 - val_loss: 0.4821 - val_acc: 0.7552\n"
     ]
    }
   ],
   "source": [
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5d555e1bd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll build a with two hidden layers, each with 6 nodes, \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer, with a learning rate of .003 and training for for 1500 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1500\n",
      "576/576 [==============================] - 0s 405us/step - loss: 0.6825 - acc: 0.5226 - val_loss: 0.7027 - val_acc: 0.4844\n",
      "Epoch 2/1500\n",
      "576/576 [==============================] - 0s 114us/step - loss: 0.6796 - acc: 0.5347 - val_loss: 0.6995 - val_acc: 0.4896\n",
      "Epoch 3/1500\n",
      "576/576 [==============================] - 0s 121us/step - loss: 0.6768 - acc: 0.5399 - val_loss: 0.6963 - val_acc: 0.5156\n",
      "Epoch 4/1500\n",
      "576/576 [==============================] - 0s 179us/step - loss: 0.6740 - acc: 0.5503 - val_loss: 0.6933 - val_acc: 0.5260\n",
      "Epoch 5/1500\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.6713 - acc: 0.5590 - val_loss: 0.6902 - val_acc: 0.5312\n",
      "Epoch 6/1500\n",
      "576/576 [==============================] - 0s 111us/step - loss: 0.6686 - acc: 0.5694 - val_loss: 0.6872 - val_acc: 0.5312\n",
      "Epoch 7/1500\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.6660 - acc: 0.5712 - val_loss: 0.6843 - val_acc: 0.5312\n",
      "Epoch 8/1500\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.6634 - acc: 0.5833 - val_loss: 0.6815 - val_acc: 0.5573\n",
      "Epoch 9/1500\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.6608 - acc: 0.5955 - val_loss: 0.6788 - val_acc: 0.5625\n",
      "Epoch 10/1500\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.6583 - acc: 0.5990 - val_loss: 0.6761 - val_acc: 0.5677\n",
      "Epoch 11/1500\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.6558 - acc: 0.6024 - val_loss: 0.6735 - val_acc: 0.5781\n",
      "Epoch 12/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.6533 - acc: 0.6146 - val_loss: 0.6709 - val_acc: 0.5729\n",
      "Epoch 13/1500\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.6509 - acc: 0.6233 - val_loss: 0.6684 - val_acc: 0.5781\n",
      "Epoch 14/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6486 - acc: 0.6250 - val_loss: 0.6659 - val_acc: 0.5938\n",
      "Epoch 15/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.6463 - acc: 0.6319 - val_loss: 0.6634 - val_acc: 0.5938\n",
      "Epoch 16/1500\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.6439 - acc: 0.6389 - val_loss: 0.6610 - val_acc: 0.5990\n",
      "Epoch 17/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6417 - acc: 0.6441 - val_loss: 0.6586 - val_acc: 0.5938\n",
      "Epoch 18/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.6395 - acc: 0.6424 - val_loss: 0.6563 - val_acc: 0.6094\n",
      "Epoch 19/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6373 - acc: 0.6476 - val_loss: 0.6540 - val_acc: 0.6198\n",
      "Epoch 20/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6352 - acc: 0.6493 - val_loss: 0.6518 - val_acc: 0.6302\n",
      "Epoch 21/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6330 - acc: 0.6545 - val_loss: 0.6496 - val_acc: 0.6302\n",
      "Epoch 22/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6310 - acc: 0.6597 - val_loss: 0.6475 - val_acc: 0.6250\n",
      "Epoch 23/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6289 - acc: 0.6632 - val_loss: 0.6454 - val_acc: 0.6146\n",
      "Epoch 24/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6269 - acc: 0.6667 - val_loss: 0.6434 - val_acc: 0.6302\n",
      "Epoch 25/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6249 - acc: 0.6736 - val_loss: 0.6414 - val_acc: 0.6406\n",
      "Epoch 26/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6230 - acc: 0.6736 - val_loss: 0.6395 - val_acc: 0.6406\n",
      "Epoch 27/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6211 - acc: 0.6753 - val_loss: 0.6377 - val_acc: 0.6406\n",
      "Epoch 28/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.6192 - acc: 0.6806 - val_loss: 0.6358 - val_acc: 0.6302\n",
      "Epoch 29/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6174 - acc: 0.6875 - val_loss: 0.6340 - val_acc: 0.6302\n",
      "Epoch 30/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6156 - acc: 0.6875 - val_loss: 0.6322 - val_acc: 0.6302\n",
      "Epoch 31/1500\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6138 - acc: 0.6892 - val_loss: 0.6305 - val_acc: 0.6406\n",
      "Epoch 32/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6120 - acc: 0.6927 - val_loss: 0.6287 - val_acc: 0.6510\n",
      "Epoch 33/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6102 - acc: 0.6944 - val_loss: 0.6270 - val_acc: 0.6510\n",
      "Epoch 34/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6085 - acc: 0.6962 - val_loss: 0.6252 - val_acc: 0.6562\n",
      "Epoch 35/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6067 - acc: 0.6962 - val_loss: 0.6235 - val_acc: 0.6562\n",
      "Epoch 36/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6050 - acc: 0.6997 - val_loss: 0.6218 - val_acc: 0.6615\n",
      "Epoch 37/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6033 - acc: 0.7014 - val_loss: 0.6201 - val_acc: 0.6667\n",
      "Epoch 38/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6017 - acc: 0.7049 - val_loss: 0.6184 - val_acc: 0.6667\n",
      "Epoch 39/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6001 - acc: 0.7049 - val_loss: 0.6168 - val_acc: 0.6719\n",
      "Epoch 40/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5985 - acc: 0.7031 - val_loss: 0.6152 - val_acc: 0.6667\n",
      "Epoch 41/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5970 - acc: 0.7031 - val_loss: 0.6137 - val_acc: 0.6615\n",
      "Epoch 42/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5955 - acc: 0.7014 - val_loss: 0.6121 - val_acc: 0.6667\n",
      "Epoch 43/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5940 - acc: 0.6979 - val_loss: 0.6106 - val_acc: 0.6667\n",
      "Epoch 44/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5925 - acc: 0.7014 - val_loss: 0.6092 - val_acc: 0.6719\n",
      "Epoch 45/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5911 - acc: 0.7049 - val_loss: 0.6077 - val_acc: 0.6823\n",
      "Epoch 46/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5896 - acc: 0.7101 - val_loss: 0.6063 - val_acc: 0.6927\n",
      "Epoch 47/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5882 - acc: 0.7101 - val_loss: 0.6049 - val_acc: 0.6979\n",
      "Epoch 48/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5868 - acc: 0.7188 - val_loss: 0.6035 - val_acc: 0.7031\n",
      "Epoch 49/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5855 - acc: 0.7222 - val_loss: 0.6022 - val_acc: 0.7188\n",
      "Epoch 50/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5841 - acc: 0.7240 - val_loss: 0.6008 - val_acc: 0.7188\n",
      "Epoch 51/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5828 - acc: 0.7257 - val_loss: 0.5995 - val_acc: 0.7292\n",
      "Epoch 52/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5816 - acc: 0.7240 - val_loss: 0.5982 - val_acc: 0.7292\n",
      "Epoch 53/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5803 - acc: 0.7309 - val_loss: 0.5969 - val_acc: 0.7292\n",
      "Epoch 54/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5791 - acc: 0.7274 - val_loss: 0.5957 - val_acc: 0.7344\n",
      "Epoch 55/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5779 - acc: 0.7309 - val_loss: 0.5944 - val_acc: 0.7396\n",
      "Epoch 56/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5766 - acc: 0.7309 - val_loss: 0.5932 - val_acc: 0.7448\n",
      "Epoch 57/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5754 - acc: 0.7326 - val_loss: 0.5920 - val_acc: 0.7448\n",
      "Epoch 58/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5743 - acc: 0.7309 - val_loss: 0.5908 - val_acc: 0.7448\n",
      "Epoch 59/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5731 - acc: 0.7309 - val_loss: 0.5896 - val_acc: 0.7448\n",
      "Epoch 60/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5720 - acc: 0.7309 - val_loss: 0.5884 - val_acc: 0.7500\n",
      "Epoch 61/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5708 - acc: 0.7309 - val_loss: 0.5873 - val_acc: 0.7500\n",
      "Epoch 62/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5697 - acc: 0.7292 - val_loss: 0.5861 - val_acc: 0.7552\n",
      "Epoch 63/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5686 - acc: 0.7309 - val_loss: 0.5850 - val_acc: 0.7552\n",
      "Epoch 64/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5675 - acc: 0.7344 - val_loss: 0.5839 - val_acc: 0.7552\n",
      "Epoch 65/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5664 - acc: 0.7309 - val_loss: 0.5828 - val_acc: 0.7552\n",
      "Epoch 66/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5653 - acc: 0.7344 - val_loss: 0.5818 - val_acc: 0.7552\n",
      "Epoch 67/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5643 - acc: 0.7344 - val_loss: 0.5807 - val_acc: 0.7552\n",
      "Epoch 68/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5632 - acc: 0.7344 - val_loss: 0.5797 - val_acc: 0.7552\n",
      "Epoch 69/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5622 - acc: 0.7361 - val_loss: 0.5786 - val_acc: 0.7552\n",
      "Epoch 70/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5611 - acc: 0.7378 - val_loss: 0.5776 - val_acc: 0.7604\n",
      "Epoch 71/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5601 - acc: 0.7396 - val_loss: 0.5766 - val_acc: 0.7656\n",
      "Epoch 72/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5591 - acc: 0.7413 - val_loss: 0.5756 - val_acc: 0.7604\n",
      "Epoch 73/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5581 - acc: 0.7413 - val_loss: 0.5747 - val_acc: 0.7604\n",
      "Epoch 74/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5571 - acc: 0.7378 - val_loss: 0.5737 - val_acc: 0.7604\n",
      "Epoch 75/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5561 - acc: 0.7396 - val_loss: 0.5728 - val_acc: 0.7604\n",
      "Epoch 76/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5551 - acc: 0.7396 - val_loss: 0.5718 - val_acc: 0.7604\n",
      "Epoch 77/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5541 - acc: 0.7378 - val_loss: 0.5709 - val_acc: 0.7656\n",
      "Epoch 78/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5531 - acc: 0.7396 - val_loss: 0.5700 - val_acc: 0.7708\n",
      "Epoch 79/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5522 - acc: 0.7396 - val_loss: 0.5691 - val_acc: 0.7708\n",
      "Epoch 80/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5512 - acc: 0.7431 - val_loss: 0.5682 - val_acc: 0.7708\n",
      "Epoch 81/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5503 - acc: 0.7396 - val_loss: 0.5673 - val_acc: 0.7708\n",
      "Epoch 82/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5493 - acc: 0.7431 - val_loss: 0.5664 - val_acc: 0.7656\n",
      "Epoch 83/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.5484 - acc: 0.7431 - val_loss: 0.5655 - val_acc: 0.7656\n",
      "Epoch 84/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5475 - acc: 0.7431 - val_loss: 0.5646 - val_acc: 0.7656\n",
      "Epoch 85/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5466 - acc: 0.7431 - val_loss: 0.5638 - val_acc: 0.7656\n",
      "Epoch 86/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5457 - acc: 0.7431 - val_loss: 0.5629 - val_acc: 0.7656\n",
      "Epoch 87/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5448 - acc: 0.7431 - val_loss: 0.5621 - val_acc: 0.7656\n",
      "Epoch 88/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5440 - acc: 0.7431 - val_loss: 0.5612 - val_acc: 0.7656\n",
      "Epoch 89/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5431 - acc: 0.7448 - val_loss: 0.5604 - val_acc: 0.7604\n",
      "Epoch 90/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5423 - acc: 0.7431 - val_loss: 0.5596 - val_acc: 0.7604\n",
      "Epoch 91/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5415 - acc: 0.7448 - val_loss: 0.5588 - val_acc: 0.7604\n",
      "Epoch 92/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5407 - acc: 0.7448 - val_loss: 0.5581 - val_acc: 0.7604\n",
      "Epoch 93/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5400 - acc: 0.7413 - val_loss: 0.5573 - val_acc: 0.7604\n",
      "Epoch 94/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5392 - acc: 0.7413 - val_loss: 0.5565 - val_acc: 0.7552\n",
      "Epoch 95/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5384 - acc: 0.7413 - val_loss: 0.5558 - val_acc: 0.7552\n",
      "Epoch 96/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5377 - acc: 0.7413 - val_loss: 0.5550 - val_acc: 0.7552\n",
      "Epoch 97/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5369 - acc: 0.7413 - val_loss: 0.5543 - val_acc: 0.7552\n",
      "Epoch 98/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5362 - acc: 0.7431 - val_loss: 0.5535 - val_acc: 0.7552\n",
      "Epoch 99/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5355 - acc: 0.7431 - val_loss: 0.5528 - val_acc: 0.7552\n",
      "Epoch 100/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5348 - acc: 0.7431 - val_loss: 0.5521 - val_acc: 0.7604\n",
      "Epoch 101/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5341 - acc: 0.7431 - val_loss: 0.5514 - val_acc: 0.7604\n",
      "Epoch 102/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5333 - acc: 0.7413 - val_loss: 0.5507 - val_acc: 0.7604\n",
      "Epoch 103/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5326 - acc: 0.7413 - val_loss: 0.5500 - val_acc: 0.7604\n",
      "Epoch 104/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5319 - acc: 0.7448 - val_loss: 0.5493 - val_acc: 0.7604\n",
      "Epoch 105/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5312 - acc: 0.7431 - val_loss: 0.5486 - val_acc: 0.7552\n",
      "Epoch 106/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5306 - acc: 0.7431 - val_loss: 0.5479 - val_acc: 0.7500\n",
      "Epoch 107/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5299 - acc: 0.7431 - val_loss: 0.5473 - val_acc: 0.7500\n",
      "Epoch 108/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5293 - acc: 0.7448 - val_loss: 0.5466 - val_acc: 0.7500\n",
      "Epoch 109/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5286 - acc: 0.7431 - val_loss: 0.5460 - val_acc: 0.7500\n",
      "Epoch 110/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5279 - acc: 0.7448 - val_loss: 0.5453 - val_acc: 0.7500\n",
      "Epoch 111/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5273 - acc: 0.7431 - val_loss: 0.5447 - val_acc: 0.7448\n",
      "Epoch 112/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5266 - acc: 0.7431 - val_loss: 0.5440 - val_acc: 0.7448\n",
      "Epoch 113/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5260 - acc: 0.7448 - val_loss: 0.5434 - val_acc: 0.7448\n",
      "Epoch 114/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5254 - acc: 0.7431 - val_loss: 0.5427 - val_acc: 0.7448\n",
      "Epoch 115/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5247 - acc: 0.7431 - val_loss: 0.5421 - val_acc: 0.7448\n",
      "Epoch 116/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5241 - acc: 0.7431 - val_loss: 0.5415 - val_acc: 0.7448\n",
      "Epoch 117/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5235 - acc: 0.7431 - val_loss: 0.5408 - val_acc: 0.7396\n",
      "Epoch 118/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5229 - acc: 0.7448 - val_loss: 0.5402 - val_acc: 0.7396\n",
      "Epoch 119/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5223 - acc: 0.7448 - val_loss: 0.5396 - val_acc: 0.7396\n",
      "Epoch 120/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5218 - acc: 0.7448 - val_loss: 0.5391 - val_acc: 0.7448\n",
      "Epoch 121/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.5212 - acc: 0.7465 - val_loss: 0.5385 - val_acc: 0.7500\n",
      "Epoch 122/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5207 - acc: 0.7448 - val_loss: 0.5379 - val_acc: 0.7500\n",
      "Epoch 123/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5201 - acc: 0.7448 - val_loss: 0.5373 - val_acc: 0.7500\n",
      "Epoch 124/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5195 - acc: 0.7431 - val_loss: 0.5368 - val_acc: 0.7500\n",
      "Epoch 125/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5190 - acc: 0.7448 - val_loss: 0.5362 - val_acc: 0.7500\n",
      "Epoch 126/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5184 - acc: 0.7465 - val_loss: 0.5357 - val_acc: 0.7552\n",
      "Epoch 127/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5179 - acc: 0.7465 - val_loss: 0.5351 - val_acc: 0.7552\n",
      "Epoch 128/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5174 - acc: 0.7483 - val_loss: 0.5346 - val_acc: 0.7604\n",
      "Epoch 129/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5169 - acc: 0.7483 - val_loss: 0.5341 - val_acc: 0.7604\n",
      "Epoch 130/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5164 - acc: 0.7483 - val_loss: 0.5336 - val_acc: 0.7604\n",
      "Epoch 131/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5158 - acc: 0.7483 - val_loss: 0.5330 - val_acc: 0.7604\n",
      "Epoch 132/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5154 - acc: 0.7500 - val_loss: 0.5325 - val_acc: 0.7604\n",
      "Epoch 133/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5148 - acc: 0.7500 - val_loss: 0.5321 - val_acc: 0.7604\n",
      "Epoch 134/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5144 - acc: 0.7500 - val_loss: 0.5316 - val_acc: 0.7604\n",
      "Epoch 135/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5139 - acc: 0.7500 - val_loss: 0.5311 - val_acc: 0.7604\n",
      "Epoch 136/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5134 - acc: 0.7500 - val_loss: 0.5306 - val_acc: 0.7604\n",
      "Epoch 137/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5130 - acc: 0.7500 - val_loss: 0.5302 - val_acc: 0.7604\n",
      "Epoch 138/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5125 - acc: 0.7500 - val_loss: 0.5297 - val_acc: 0.7604\n",
      "Epoch 139/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5121 - acc: 0.7500 - val_loss: 0.5293 - val_acc: 0.7604\n",
      "Epoch 140/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5116 - acc: 0.7500 - val_loss: 0.5288 - val_acc: 0.7604\n",
      "Epoch 141/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5111 - acc: 0.7500 - val_loss: 0.5284 - val_acc: 0.7604\n",
      "Epoch 142/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5106 - acc: 0.7483 - val_loss: 0.5279 - val_acc: 0.7604\n",
      "Epoch 143/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5102 - acc: 0.7500 - val_loss: 0.5275 - val_acc: 0.7604\n",
      "Epoch 144/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5097 - acc: 0.7483 - val_loss: 0.5270 - val_acc: 0.7656\n",
      "Epoch 145/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5093 - acc: 0.7500 - val_loss: 0.5266 - val_acc: 0.7656\n",
      "Epoch 146/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5088 - acc: 0.7483 - val_loss: 0.5261 - val_acc: 0.7708\n",
      "Epoch 147/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5084 - acc: 0.7465 - val_loss: 0.5257 - val_acc: 0.7708\n",
      "Epoch 148/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5079 - acc: 0.7465 - val_loss: 0.5253 - val_acc: 0.7708\n",
      "Epoch 149/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5075 - acc: 0.7465 - val_loss: 0.5248 - val_acc: 0.7708\n",
      "Epoch 150/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5070 - acc: 0.7483 - val_loss: 0.5244 - val_acc: 0.7760\n",
      "Epoch 151/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5066 - acc: 0.7483 - val_loss: 0.5240 - val_acc: 0.7760\n",
      "Epoch 152/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5062 - acc: 0.7483 - val_loss: 0.5236 - val_acc: 0.7760\n",
      "Epoch 153/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5057 - acc: 0.7500 - val_loss: 0.5232 - val_acc: 0.7760\n",
      "Epoch 154/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5053 - acc: 0.7500 - val_loss: 0.5228 - val_acc: 0.7760\n",
      "Epoch 155/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5049 - acc: 0.7500 - val_loss: 0.5223 - val_acc: 0.7760\n",
      "Epoch 156/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5045 - acc: 0.7500 - val_loss: 0.5219 - val_acc: 0.7760\n",
      "Epoch 157/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5041 - acc: 0.7517 - val_loss: 0.5215 - val_acc: 0.7812\n",
      "Epoch 158/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5037 - acc: 0.7500 - val_loss: 0.5211 - val_acc: 0.7812\n",
      "Epoch 159/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5033 - acc: 0.7535 - val_loss: 0.5207 - val_acc: 0.7760\n",
      "Epoch 160/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5030 - acc: 0.7517 - val_loss: 0.5203 - val_acc: 0.7760\n",
      "Epoch 161/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5026 - acc: 0.7535 - val_loss: 0.5199 - val_acc: 0.7760\n",
      "Epoch 162/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5022 - acc: 0.7535 - val_loss: 0.5195 - val_acc: 0.7760\n",
      "Epoch 163/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5018 - acc: 0.7535 - val_loss: 0.5191 - val_acc: 0.7760\n",
      "Epoch 164/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5014 - acc: 0.7517 - val_loss: 0.5187 - val_acc: 0.7760\n",
      "Epoch 165/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5011 - acc: 0.7535 - val_loss: 0.5184 - val_acc: 0.7760\n",
      "Epoch 166/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5007 - acc: 0.7535 - val_loss: 0.5180 - val_acc: 0.7760\n",
      "Epoch 167/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5004 - acc: 0.7535 - val_loss: 0.5176 - val_acc: 0.7760\n",
      "Epoch 168/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5000 - acc: 0.7535 - val_loss: 0.5173 - val_acc: 0.7760\n",
      "Epoch 169/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4996 - acc: 0.7535 - val_loss: 0.5169 - val_acc: 0.7760\n",
      "Epoch 170/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4993 - acc: 0.7535 - val_loss: 0.5166 - val_acc: 0.7760\n",
      "Epoch 171/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4989 - acc: 0.7535 - val_loss: 0.5162 - val_acc: 0.7812\n",
      "Epoch 172/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4986 - acc: 0.7535 - val_loss: 0.5159 - val_acc: 0.7865\n",
      "Epoch 173/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4982 - acc: 0.7552 - val_loss: 0.5155 - val_acc: 0.7865\n",
      "Epoch 174/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4979 - acc: 0.7535 - val_loss: 0.5152 - val_acc: 0.7865\n",
      "Epoch 175/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4976 - acc: 0.7552 - val_loss: 0.5148 - val_acc: 0.7865\n",
      "Epoch 176/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4972 - acc: 0.7569 - val_loss: 0.5145 - val_acc: 0.7865\n",
      "Epoch 177/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4969 - acc: 0.7569 - val_loss: 0.5141 - val_acc: 0.7865\n",
      "Epoch 178/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4966 - acc: 0.7569 - val_loss: 0.5138 - val_acc: 0.7865\n",
      "Epoch 179/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4962 - acc: 0.7569 - val_loss: 0.5135 - val_acc: 0.7865\n",
      "Epoch 180/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4959 - acc: 0.7587 - val_loss: 0.5131 - val_acc: 0.7865\n",
      "Epoch 181/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4955 - acc: 0.7569 - val_loss: 0.5128 - val_acc: 0.7865\n",
      "Epoch 182/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4952 - acc: 0.7569 - val_loss: 0.5124 - val_acc: 0.7812\n",
      "Epoch 183/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4949 - acc: 0.7569 - val_loss: 0.5121 - val_acc: 0.7812\n",
      "Epoch 184/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4946 - acc: 0.7569 - val_loss: 0.5118 - val_acc: 0.7812\n",
      "Epoch 185/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4943 - acc: 0.7552 - val_loss: 0.5114 - val_acc: 0.7812\n",
      "Epoch 186/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4940 - acc: 0.7552 - val_loss: 0.5111 - val_acc: 0.7812\n",
      "Epoch 187/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4937 - acc: 0.7569 - val_loss: 0.5108 - val_acc: 0.7812\n",
      "Epoch 188/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4933 - acc: 0.7552 - val_loss: 0.5104 - val_acc: 0.7760\n",
      "Epoch 189/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4930 - acc: 0.7552 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 190/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4927 - acc: 0.7552 - val_loss: 0.5098 - val_acc: 0.7760\n",
      "Epoch 191/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4924 - acc: 0.7552 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 192/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4921 - acc: 0.7552 - val_loss: 0.5092 - val_acc: 0.7760\n",
      "Epoch 193/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4918 - acc: 0.7552 - val_loss: 0.5089 - val_acc: 0.7760\n",
      "Epoch 194/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4915 - acc: 0.7552 - val_loss: 0.5086 - val_acc: 0.7760\n",
      "Epoch 195/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4913 - acc: 0.7552 - val_loss: 0.5083 - val_acc: 0.7760\n",
      "Epoch 196/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4910 - acc: 0.7569 - val_loss: 0.5080 - val_acc: 0.7760\n",
      "Epoch 197/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4907 - acc: 0.7569 - val_loss: 0.5077 - val_acc: 0.7760\n",
      "Epoch 198/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4904 - acc: 0.7569 - val_loss: 0.5074 - val_acc: 0.7760\n",
      "Epoch 199/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4902 - acc: 0.7569 - val_loss: 0.5071 - val_acc: 0.7760\n",
      "Epoch 200/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4899 - acc: 0.7569 - val_loss: 0.5068 - val_acc: 0.7760\n",
      "Epoch 201/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4896 - acc: 0.7569 - val_loss: 0.5065 - val_acc: 0.7760\n",
      "Epoch 202/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4894 - acc: 0.7587 - val_loss: 0.5062 - val_acc: 0.7760\n",
      "Epoch 203/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4891 - acc: 0.7569 - val_loss: 0.5059 - val_acc: 0.7760\n",
      "Epoch 204/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4888 - acc: 0.7587 - val_loss: 0.5056 - val_acc: 0.7760\n",
      "Epoch 205/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4885 - acc: 0.7587 - val_loss: 0.5053 - val_acc: 0.7760\n",
      "Epoch 206/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4883 - acc: 0.7587 - val_loss: 0.5050 - val_acc: 0.7760\n",
      "Epoch 207/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4881 - acc: 0.7569 - val_loss: 0.5047 - val_acc: 0.7760\n",
      "Epoch 208/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4878 - acc: 0.7569 - val_loss: 0.5044 - val_acc: 0.7760\n",
      "Epoch 209/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4875 - acc: 0.7569 - val_loss: 0.5041 - val_acc: 0.7760\n",
      "Epoch 210/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4873 - acc: 0.7569 - val_loss: 0.5038 - val_acc: 0.7760\n",
      "Epoch 211/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4870 - acc: 0.7569 - val_loss: 0.5036 - val_acc: 0.7760\n",
      "Epoch 212/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4868 - acc: 0.7569 - val_loss: 0.5033 - val_acc: 0.7760\n",
      "Epoch 213/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4865 - acc: 0.7552 - val_loss: 0.5030 - val_acc: 0.7760\n",
      "Epoch 214/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4863 - acc: 0.7569 - val_loss: 0.5027 - val_acc: 0.7760\n",
      "Epoch 215/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4860 - acc: 0.7587 - val_loss: 0.5025 - val_acc: 0.7760\n",
      "Epoch 216/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4858 - acc: 0.7587 - val_loss: 0.5022 - val_acc: 0.7760\n",
      "Epoch 217/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4856 - acc: 0.7587 - val_loss: 0.5019 - val_acc: 0.7760\n",
      "Epoch 218/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4853 - acc: 0.7604 - val_loss: 0.5017 - val_acc: 0.7760\n",
      "Epoch 219/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4851 - acc: 0.7569 - val_loss: 0.5014 - val_acc: 0.7760\n",
      "Epoch 220/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4849 - acc: 0.7587 - val_loss: 0.5011 - val_acc: 0.7760\n",
      "Epoch 221/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4846 - acc: 0.7604 - val_loss: 0.5009 - val_acc: 0.7760\n",
      "Epoch 222/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4844 - acc: 0.7604 - val_loss: 0.5006 - val_acc: 0.7760\n",
      "Epoch 223/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4842 - acc: 0.7604 - val_loss: 0.5004 - val_acc: 0.7760\n",
      "Epoch 224/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4840 - acc: 0.7587 - val_loss: 0.5001 - val_acc: 0.7760\n",
      "Epoch 225/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4838 - acc: 0.7587 - val_loss: 0.4998 - val_acc: 0.7760\n",
      "Epoch 226/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4835 - acc: 0.7587 - val_loss: 0.4996 - val_acc: 0.7760\n",
      "Epoch 227/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4834 - acc: 0.7587 - val_loss: 0.4993 - val_acc: 0.7760\n",
      "Epoch 228/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4831 - acc: 0.7587 - val_loss: 0.4991 - val_acc: 0.7760\n",
      "Epoch 229/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4829 - acc: 0.7587 - val_loss: 0.4988 - val_acc: 0.7760\n",
      "Epoch 230/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4827 - acc: 0.7587 - val_loss: 0.4986 - val_acc: 0.7760\n",
      "Epoch 231/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4825 - acc: 0.7587 - val_loss: 0.4983 - val_acc: 0.7760\n",
      "Epoch 232/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4822 - acc: 0.7587 - val_loss: 0.4981 - val_acc: 0.7760\n",
      "Epoch 233/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4820 - acc: 0.7604 - val_loss: 0.4978 - val_acc: 0.7760\n",
      "Epoch 234/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4819 - acc: 0.7587 - val_loss: 0.4976 - val_acc: 0.7760\n",
      "Epoch 235/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4816 - acc: 0.7604 - val_loss: 0.4973 - val_acc: 0.7708\n",
      "Epoch 236/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4814 - acc: 0.7622 - val_loss: 0.4971 - val_acc: 0.7708\n",
      "Epoch 237/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4812 - acc: 0.7622 - val_loss: 0.4968 - val_acc: 0.7708\n",
      "Epoch 238/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4810 - acc: 0.7622 - val_loss: 0.4966 - val_acc: 0.7708\n",
      "Epoch 239/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4807 - acc: 0.7639 - val_loss: 0.4963 - val_acc: 0.7708\n",
      "Epoch 240/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4805 - acc: 0.7639 - val_loss: 0.4961 - val_acc: 0.7708\n",
      "Epoch 241/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4803 - acc: 0.7639 - val_loss: 0.4959 - val_acc: 0.7708\n",
      "Epoch 242/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4801 - acc: 0.7639 - val_loss: 0.4957 - val_acc: 0.7708\n",
      "Epoch 243/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4799 - acc: 0.7639 - val_loss: 0.4955 - val_acc: 0.7708\n",
      "Epoch 244/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4797 - acc: 0.7639 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 245/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4795 - acc: 0.7656 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 246/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4793 - acc: 0.7674 - val_loss: 0.4949 - val_acc: 0.7656\n",
      "Epoch 247/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4791 - acc: 0.7674 - val_loss: 0.4947 - val_acc: 0.7656\n",
      "Epoch 248/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4789 - acc: 0.7674 - val_loss: 0.4945 - val_acc: 0.7708\n",
      "Epoch 249/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4787 - acc: 0.7674 - val_loss: 0.4943 - val_acc: 0.7708\n",
      "Epoch 250/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4785 - acc: 0.7674 - val_loss: 0.4941 - val_acc: 0.7708\n",
      "Epoch 251/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4783 - acc: 0.7674 - val_loss: 0.4940 - val_acc: 0.7708\n",
      "Epoch 252/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4781 - acc: 0.7674 - val_loss: 0.4938 - val_acc: 0.7708\n",
      "Epoch 253/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4779 - acc: 0.7656 - val_loss: 0.4936 - val_acc: 0.7708\n",
      "Epoch 254/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4777 - acc: 0.7674 - val_loss: 0.4935 - val_acc: 0.7708\n",
      "Epoch 255/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4776 - acc: 0.7674 - val_loss: 0.4933 - val_acc: 0.7708\n",
      "Epoch 256/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4774 - acc: 0.7674 - val_loss: 0.4931 - val_acc: 0.7708\n",
      "Epoch 257/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4772 - acc: 0.7674 - val_loss: 0.4930 - val_acc: 0.7708\n",
      "Epoch 258/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4770 - acc: 0.7674 - val_loss: 0.4928 - val_acc: 0.7708\n",
      "Epoch 259/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4769 - acc: 0.7674 - val_loss: 0.4927 - val_acc: 0.7708\n",
      "Epoch 260/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4767 - acc: 0.7674 - val_loss: 0.4925 - val_acc: 0.7708\n",
      "Epoch 261/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4765 - acc: 0.7674 - val_loss: 0.4924 - val_acc: 0.7708\n",
      "Epoch 262/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4764 - acc: 0.7674 - val_loss: 0.4922 - val_acc: 0.7708\n",
      "Epoch 263/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4762 - acc: 0.7674 - val_loss: 0.4920 - val_acc: 0.7708\n",
      "Epoch 264/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4760 - acc: 0.7674 - val_loss: 0.4919 - val_acc: 0.7708\n",
      "Epoch 265/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4759 - acc: 0.7674 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 266/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4757 - acc: 0.7674 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 267/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4755 - acc: 0.7674 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 268/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4754 - acc: 0.7691 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 269/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4752 - acc: 0.7691 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 270/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4750 - acc: 0.7691 - val_loss: 0.4910 - val_acc: 0.7708\n",
      "Epoch 271/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4749 - acc: 0.7691 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 272/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4747 - acc: 0.7691 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 273/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4746 - acc: 0.7691 - val_loss: 0.4906 - val_acc: 0.7708\n",
      "Epoch 274/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4744 - acc: 0.7691 - val_loss: 0.4904 - val_acc: 0.7760\n",
      "Epoch 275/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4742 - acc: 0.7691 - val_loss: 0.4903 - val_acc: 0.7760\n",
      "Epoch 276/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4741 - acc: 0.7708 - val_loss: 0.4902 - val_acc: 0.7760\n",
      "Epoch 277/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4739 - acc: 0.7708 - val_loss: 0.4900 - val_acc: 0.7760\n",
      "Epoch 278/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4737 - acc: 0.7708 - val_loss: 0.4899 - val_acc: 0.7760\n",
      "Epoch 279/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4736 - acc: 0.7708 - val_loss: 0.4898 - val_acc: 0.7760\n",
      "Epoch 280/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4734 - acc: 0.7708 - val_loss: 0.4897 - val_acc: 0.7760\n",
      "Epoch 281/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4733 - acc: 0.7691 - val_loss: 0.4895 - val_acc: 0.7760\n",
      "Epoch 282/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4731 - acc: 0.7691 - val_loss: 0.4894 - val_acc: 0.7760\n",
      "Epoch 283/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4730 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7760\n",
      "Epoch 284/1500\n",
      "576/576 [==============================] - 0s 118us/step - loss: 0.4729 - acc: 0.7708 - val_loss: 0.4892 - val_acc: 0.7760\n",
      "Epoch 285/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4728 - acc: 0.7691 - val_loss: 0.4891 - val_acc: 0.7760\n",
      "Epoch 286/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4726 - acc: 0.7691 - val_loss: 0.4889 - val_acc: 0.7760\n",
      "Epoch 287/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4725 - acc: 0.7691 - val_loss: 0.4888 - val_acc: 0.7760\n",
      "Epoch 288/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4723 - acc: 0.7691 - val_loss: 0.4887 - val_acc: 0.7760\n",
      "Epoch 289/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4722 - acc: 0.7691 - val_loss: 0.4886 - val_acc: 0.7760\n",
      "Epoch 290/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4721 - acc: 0.7691 - val_loss: 0.4885 - val_acc: 0.7760\n",
      "Epoch 291/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4719 - acc: 0.7674 - val_loss: 0.4884 - val_acc: 0.7760\n",
      "Epoch 292/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4718 - acc: 0.7674 - val_loss: 0.4883 - val_acc: 0.7760\n",
      "Epoch 293/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4717 - acc: 0.7674 - val_loss: 0.4882 - val_acc: 0.7760\n",
      "Epoch 294/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4716 - acc: 0.7674 - val_loss: 0.4881 - val_acc: 0.7760\n",
      "Epoch 295/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4714 - acc: 0.7674 - val_loss: 0.4880 - val_acc: 0.7760\n",
      "Epoch 296/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4713 - acc: 0.7674 - val_loss: 0.4879 - val_acc: 0.7760\n",
      "Epoch 297/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4712 - acc: 0.7691 - val_loss: 0.4878 - val_acc: 0.7760\n",
      "Epoch 298/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4711 - acc: 0.7691 - val_loss: 0.4877 - val_acc: 0.7760\n",
      "Epoch 299/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4710 - acc: 0.7691 - val_loss: 0.4876 - val_acc: 0.7760\n",
      "Epoch 300/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4708 - acc: 0.7691 - val_loss: 0.4875 - val_acc: 0.7760\n",
      "Epoch 301/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4707 - acc: 0.7691 - val_loss: 0.4874 - val_acc: 0.7760\n",
      "Epoch 302/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4706 - acc: 0.7691 - val_loss: 0.4873 - val_acc: 0.7760\n",
      "Epoch 303/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4704 - acc: 0.7691 - val_loss: 0.4872 - val_acc: 0.7760\n",
      "Epoch 304/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4704 - acc: 0.7691 - val_loss: 0.4871 - val_acc: 0.7760\n",
      "Epoch 305/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4702 - acc: 0.7691 - val_loss: 0.4870 - val_acc: 0.7760\n",
      "Epoch 306/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4701 - acc: 0.7691 - val_loss: 0.4869 - val_acc: 0.7760\n",
      "Epoch 307/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4700 - acc: 0.7691 - val_loss: 0.4868 - val_acc: 0.7760\n",
      "Epoch 308/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4699 - acc: 0.7691 - val_loss: 0.4867 - val_acc: 0.7760\n",
      "Epoch 309/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4698 - acc: 0.7691 - val_loss: 0.4866 - val_acc: 0.7760\n",
      "Epoch 310/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4696 - acc: 0.7691 - val_loss: 0.4865 - val_acc: 0.7760\n",
      "Epoch 311/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4695 - acc: 0.7691 - val_loss: 0.4864 - val_acc: 0.7760\n",
      "Epoch 312/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4694 - acc: 0.7708 - val_loss: 0.4863 - val_acc: 0.7760\n",
      "Epoch 313/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4693 - acc: 0.7691 - val_loss: 0.4862 - val_acc: 0.7760\n",
      "Epoch 314/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4692 - acc: 0.7691 - val_loss: 0.4861 - val_acc: 0.7760\n",
      "Epoch 315/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4691 - acc: 0.7691 - val_loss: 0.4860 - val_acc: 0.7760\n",
      "Epoch 316/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4689 - acc: 0.7691 - val_loss: 0.4860 - val_acc: 0.7760\n",
      "Epoch 317/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4688 - acc: 0.7708 - val_loss: 0.4859 - val_acc: 0.7760\n",
      "Epoch 318/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4687 - acc: 0.7708 - val_loss: 0.4858 - val_acc: 0.7760\n",
      "Epoch 319/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4686 - acc: 0.7708 - val_loss: 0.4857 - val_acc: 0.7760\n",
      "Epoch 320/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4685 - acc: 0.7726 - val_loss: 0.4856 - val_acc: 0.7760\n",
      "Epoch 321/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4683 - acc: 0.7726 - val_loss: 0.4855 - val_acc: 0.7760\n",
      "Epoch 322/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4683 - acc: 0.7726 - val_loss: 0.4854 - val_acc: 0.7760\n",
      "Epoch 323/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4681 - acc: 0.7726 - val_loss: 0.4853 - val_acc: 0.7760\n",
      "Epoch 324/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4680 - acc: 0.7726 - val_loss: 0.4853 - val_acc: 0.7760\n",
      "Epoch 325/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4679 - acc: 0.7726 - val_loss: 0.4852 - val_acc: 0.7760\n",
      "Epoch 326/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4678 - acc: 0.7726 - val_loss: 0.4851 - val_acc: 0.7760\n",
      "Epoch 327/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4676 - acc: 0.7726 - val_loss: 0.4850 - val_acc: 0.7760\n",
      "Epoch 328/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4675 - acc: 0.7726 - val_loss: 0.4849 - val_acc: 0.7760\n",
      "Epoch 329/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4674 - acc: 0.7743 - val_loss: 0.4849 - val_acc: 0.7760\n",
      "Epoch 330/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4673 - acc: 0.7743 - val_loss: 0.4848 - val_acc: 0.7760\n",
      "Epoch 331/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4672 - acc: 0.7743 - val_loss: 0.4847 - val_acc: 0.7760\n",
      "Epoch 332/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4671 - acc: 0.7743 - val_loss: 0.4846 - val_acc: 0.7760\n",
      "Epoch 333/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4670 - acc: 0.7743 - val_loss: 0.4846 - val_acc: 0.7760\n",
      "Epoch 334/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4668 - acc: 0.7743 - val_loss: 0.4845 - val_acc: 0.7760\n",
      "Epoch 335/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4668 - acc: 0.7743 - val_loss: 0.4844 - val_acc: 0.7708\n",
      "Epoch 336/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4666 - acc: 0.7743 - val_loss: 0.4844 - val_acc: 0.7708\n",
      "Epoch 337/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4665 - acc: 0.7743 - val_loss: 0.4843 - val_acc: 0.7708\n",
      "Epoch 338/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4664 - acc: 0.7726 - val_loss: 0.4842 - val_acc: 0.7708\n",
      "Epoch 339/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4663 - acc: 0.7726 - val_loss: 0.4841 - val_acc: 0.7708\n",
      "Epoch 340/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4662 - acc: 0.7743 - val_loss: 0.4841 - val_acc: 0.7708\n",
      "Epoch 341/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4661 - acc: 0.7743 - val_loss: 0.4840 - val_acc: 0.7708\n",
      "Epoch 342/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4659 - acc: 0.7743 - val_loss: 0.4839 - val_acc: 0.7708\n",
      "Epoch 343/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4658 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7708\n",
      "Epoch 344/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4657 - acc: 0.7743 - val_loss: 0.4838 - val_acc: 0.7708\n",
      "Epoch 345/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4656 - acc: 0.7743 - val_loss: 0.4837 - val_acc: 0.7708\n",
      "Epoch 346/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4655 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7708\n",
      "Epoch 347/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4654 - acc: 0.7743 - val_loss: 0.4836 - val_acc: 0.7708\n",
      "Epoch 348/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4653 - acc: 0.7743 - val_loss: 0.4835 - val_acc: 0.7708\n",
      "Epoch 349/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4652 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7708\n",
      "Epoch 350/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4650 - acc: 0.7743 - val_loss: 0.4834 - val_acc: 0.7708\n",
      "Epoch 351/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4649 - acc: 0.7743 - val_loss: 0.4833 - val_acc: 0.7708\n",
      "Epoch 352/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4648 - acc: 0.7743 - val_loss: 0.4832 - val_acc: 0.7708\n",
      "Epoch 353/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4647 - acc: 0.7743 - val_loss: 0.4832 - val_acc: 0.7708\n",
      "Epoch 354/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4646 - acc: 0.7743 - val_loss: 0.4831 - val_acc: 0.7708\n",
      "Epoch 355/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4645 - acc: 0.7743 - val_loss: 0.4830 - val_acc: 0.7708\n",
      "Epoch 356/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4643 - acc: 0.7743 - val_loss: 0.4830 - val_acc: 0.7708\n",
      "Epoch 357/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4642 - acc: 0.7743 - val_loss: 0.4829 - val_acc: 0.7708\n",
      "Epoch 358/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4641 - acc: 0.7743 - val_loss: 0.4828 - val_acc: 0.7708\n",
      "Epoch 359/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4640 - acc: 0.7743 - val_loss: 0.4828 - val_acc: 0.7708\n",
      "Epoch 360/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4639 - acc: 0.7743 - val_loss: 0.4827 - val_acc: 0.7708\n",
      "Epoch 361/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4638 - acc: 0.7743 - val_loss: 0.4827 - val_acc: 0.7708\n",
      "Epoch 362/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4637 - acc: 0.7726 - val_loss: 0.4826 - val_acc: 0.7708\n",
      "Epoch 363/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4635 - acc: 0.7743 - val_loss: 0.4826 - val_acc: 0.7708\n",
      "Epoch 364/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4635 - acc: 0.7743 - val_loss: 0.4825 - val_acc: 0.7708\n",
      "Epoch 365/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4633 - acc: 0.7743 - val_loss: 0.4825 - val_acc: 0.7604\n",
      "Epoch 366/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4632 - acc: 0.7726 - val_loss: 0.4824 - val_acc: 0.7604\n",
      "Epoch 367/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4631 - acc: 0.7726 - val_loss: 0.4824 - val_acc: 0.7604\n",
      "Epoch 368/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4630 - acc: 0.7743 - val_loss: 0.4823 - val_acc: 0.7604\n",
      "Epoch 369/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4629 - acc: 0.7743 - val_loss: 0.4823 - val_acc: 0.7604\n",
      "Epoch 370/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4628 - acc: 0.7743 - val_loss: 0.4822 - val_acc: 0.7604\n",
      "Epoch 371/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4627 - acc: 0.7743 - val_loss: 0.4822 - val_acc: 0.7604\n",
      "Epoch 372/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4626 - acc: 0.7743 - val_loss: 0.4821 - val_acc: 0.7604\n",
      "Epoch 373/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4625 - acc: 0.7743 - val_loss: 0.4821 - val_acc: 0.7604\n",
      "Epoch 374/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4623 - acc: 0.7760 - val_loss: 0.4820 - val_acc: 0.7604\n",
      "Epoch 375/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4622 - acc: 0.7760 - val_loss: 0.4820 - val_acc: 0.7604\n",
      "Epoch 376/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4621 - acc: 0.7760 - val_loss: 0.4820 - val_acc: 0.7604\n",
      "Epoch 377/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4819 - val_acc: 0.7604\n",
      "Epoch 378/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4819 - val_acc: 0.7604\n",
      "Epoch 379/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4818 - val_acc: 0.7604\n",
      "Epoch 380/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4818 - val_acc: 0.7604\n",
      "Epoch 381/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4616 - acc: 0.7760 - val_loss: 0.4817 - val_acc: 0.7604\n",
      "Epoch 382/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4817 - val_acc: 0.7604\n",
      "Epoch 383/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4816 - val_acc: 0.7604\n",
      "Epoch 384/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4816 - val_acc: 0.7604\n",
      "Epoch 385/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4815 - val_acc: 0.7604\n",
      "Epoch 386/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4815 - val_acc: 0.7604\n",
      "Epoch 387/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4609 - acc: 0.7743 - val_loss: 0.4815 - val_acc: 0.7656\n",
      "Epoch 388/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4609 - acc: 0.7760 - val_loss: 0.4814 - val_acc: 0.7656\n",
      "Epoch 389/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4608 - acc: 0.7760 - val_loss: 0.4814 - val_acc: 0.7656\n",
      "Epoch 390/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4607 - acc: 0.7760 - val_loss: 0.4813 - val_acc: 0.7656\n",
      "Epoch 391/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4606 - acc: 0.7760 - val_loss: 0.4813 - val_acc: 0.7656\n",
      "Epoch 392/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4605 - acc: 0.7760 - val_loss: 0.4812 - val_acc: 0.7656\n",
      "Epoch 393/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4604 - acc: 0.7760 - val_loss: 0.4812 - val_acc: 0.7656\n",
      "Epoch 394/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4603 - acc: 0.7760 - val_loss: 0.4812 - val_acc: 0.7656\n",
      "Epoch 395/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4601 - acc: 0.7760 - val_loss: 0.4811 - val_acc: 0.7656\n",
      "Epoch 396/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4601 - acc: 0.7760 - val_loss: 0.4811 - val_acc: 0.7656\n",
      "Epoch 397/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4600 - acc: 0.7760 - val_loss: 0.4811 - val_acc: 0.7656\n",
      "Epoch 398/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4598 - acc: 0.7760 - val_loss: 0.4810 - val_acc: 0.7656\n",
      "Epoch 399/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4597 - acc: 0.7760 - val_loss: 0.4810 - val_acc: 0.7656\n",
      "Epoch 400/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4810 - val_acc: 0.7656\n",
      "Epoch 401/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.4809 - val_acc: 0.7656\n",
      "Epoch 402/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4595 - acc: 0.7760 - val_loss: 0.4809 - val_acc: 0.7656\n",
      "Epoch 403/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4594 - acc: 0.7743 - val_loss: 0.4809 - val_acc: 0.7656\n",
      "Epoch 404/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4593 - acc: 0.7743 - val_loss: 0.4809 - val_acc: 0.7656\n",
      "Epoch 405/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4591 - acc: 0.7760 - val_loss: 0.4808 - val_acc: 0.7656\n",
      "Epoch 406/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4591 - acc: 0.7743 - val_loss: 0.4808 - val_acc: 0.7656\n",
      "Epoch 407/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4590 - acc: 0.7760 - val_loss: 0.4808 - val_acc: 0.7656\n",
      "Epoch 408/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4588 - acc: 0.7743 - val_loss: 0.4807 - val_acc: 0.7656\n",
      "Epoch 409/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.4807 - val_acc: 0.7656\n",
      "Epoch 410/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4807 - val_acc: 0.7656\n",
      "Epoch 411/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4586 - acc: 0.7760 - val_loss: 0.4807 - val_acc: 0.7656\n",
      "Epoch 412/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4584 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7656\n",
      "Epoch 413/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4584 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7656\n",
      "Epoch 414/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4583 - acc: 0.7778 - val_loss: 0.4806 - val_acc: 0.7656\n",
      "Epoch 415/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4582 - acc: 0.7760 - val_loss: 0.4805 - val_acc: 0.7656\n",
      "Epoch 416/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4580 - acc: 0.7778 - val_loss: 0.4805 - val_acc: 0.7656\n",
      "Epoch 417/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4579 - acc: 0.7778 - val_loss: 0.4805 - val_acc: 0.7656\n",
      "Epoch 418/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4579 - acc: 0.7778 - val_loss: 0.4805 - val_acc: 0.7656\n",
      "Epoch 419/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4578 - acc: 0.7760 - val_loss: 0.4804 - val_acc: 0.7656\n",
      "Epoch 420/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4577 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7656\n",
      "Epoch 421/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4576 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7656\n",
      "Epoch 422/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4575 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7656\n",
      "Epoch 423/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4574 - acc: 0.7760 - val_loss: 0.4803 - val_acc: 0.7656\n",
      "Epoch 424/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4573 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7656\n",
      "Epoch 425/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4572 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7656\n",
      "Epoch 426/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4571 - acc: 0.7760 - val_loss: 0.4803 - val_acc: 0.7656\n",
      "Epoch 427/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4570 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7656\n",
      "Epoch 428/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4569 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7656\n",
      "Epoch 429/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4568 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7656\n",
      "Epoch 430/1500\n",
      "576/576 [==============================] - 0s 132us/step - loss: 0.4567 - acc: 0.7795 - val_loss: 0.4802 - val_acc: 0.7656\n",
      "Epoch 431/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4566 - acc: 0.7795 - val_loss: 0.4802 - val_acc: 0.7708\n",
      "Epoch 432/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4566 - acc: 0.7795 - val_loss: 0.4802 - val_acc: 0.7708\n",
      "Epoch 433/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4564 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7708\n",
      "Epoch 434/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4564 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7708\n",
      "Epoch 435/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4563 - acc: 0.7795 - val_loss: 0.4801 - val_acc: 0.7708\n",
      "Epoch 436/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4562 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7656\n",
      "Epoch 437/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4561 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7656\n",
      "Epoch 438/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4560 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7656\n",
      "Epoch 439/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4559 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7656\n",
      "Epoch 440/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4558 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7656\n",
      "Epoch 441/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4558 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7656\n",
      "Epoch 442/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4557 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7656\n",
      "Epoch 443/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4556 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7656\n",
      "Epoch 444/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4555 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7656\n",
      "Epoch 445/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4554 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7656\n",
      "Epoch 446/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4554 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7656\n",
      "Epoch 447/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4553 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7656\n",
      "Epoch 448/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4552 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7656\n",
      "Epoch 449/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4551 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7656\n",
      "Epoch 450/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4550 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7656\n",
      "Epoch 451/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4550 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7656\n",
      "Epoch 452/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4549 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7656\n",
      "Epoch 453/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4548 - acc: 0.7760 - val_loss: 0.4799 - val_acc: 0.7656\n",
      "Epoch 454/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4547 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7656\n",
      "Epoch 455/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4547 - acc: 0.7795 - val_loss: 0.4799 - val_acc: 0.7656\n",
      "Epoch 456/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4546 - acc: 0.7795 - val_loss: 0.4799 - val_acc: 0.7656\n",
      "Epoch 457/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4545 - acc: 0.7795 - val_loss: 0.4798 - val_acc: 0.7656\n",
      "Epoch 458/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4544 - acc: 0.7795 - val_loss: 0.4798 - val_acc: 0.7656\n",
      "Epoch 459/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4544 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7656\n",
      "Epoch 460/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4543 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7656\n",
      "Epoch 461/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4542 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7656\n",
      "Epoch 462/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4542 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7656\n",
      "Epoch 463/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4541 - acc: 0.7760 - val_loss: 0.4797 - val_acc: 0.7656\n",
      "Epoch 464/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4540 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7656\n",
      "Epoch 465/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4540 - acc: 0.7795 - val_loss: 0.4797 - val_acc: 0.7656\n",
      "Epoch 466/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4539 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7656\n",
      "Epoch 467/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4538 - acc: 0.7760 - val_loss: 0.4797 - val_acc: 0.7656\n",
      "Epoch 468/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4537 - acc: 0.7760 - val_loss: 0.4797 - val_acc: 0.7656\n",
      "Epoch 469/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4537 - acc: 0.7760 - val_loss: 0.4797 - val_acc: 0.7656\n",
      "Epoch 470/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4536 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7604\n",
      "Epoch 471/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4535 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7604\n",
      "Epoch 472/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4535 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 473/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4534 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 474/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4533 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 475/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4533 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 476/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4532 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 477/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4531 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 478/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4531 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 479/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4530 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 480/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4529 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 481/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4529 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7604\n",
      "Epoch 482/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4528 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 483/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 484/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 485/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4526 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 486/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4525 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 487/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4525 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 488/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4524 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 489/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4523 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 490/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4523 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 491/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4522 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 492/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4521 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 493/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4521 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 494/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4520 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 495/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4520 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 496/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4519 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 497/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4518 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7604\n",
      "Epoch 498/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4518 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 499/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4517 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 500/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4516 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 501/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4516 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 502/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4515 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 503/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4515 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 504/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4514 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 505/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4513 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 506/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4513 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 507/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4512 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 508/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4512 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 509/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4511 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 510/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4511 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 511/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4510 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 512/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4510 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 513/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4509 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 514/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4509 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 515/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4508 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 516/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4507 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 517/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4507 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 518/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4507 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 519/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4506 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 520/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4505 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 521/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4505 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 522/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4504 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 523/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4504 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 524/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4503 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 525/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4503 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 526/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4502 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 527/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4501 - acc: 0.7795 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 528/1500\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4501 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 529/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4500 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 530/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4500 - acc: 0.7795 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 531/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4499 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 532/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4499 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 533/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4498 - acc: 0.7795 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 534/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4498 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 535/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4497 - acc: 0.7795 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 536/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4496 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 537/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4496 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 538/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4495 - acc: 0.7795 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 539/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4495 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 540/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4494 - acc: 0.7778 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 541/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4494 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 542/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4493 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 543/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4493 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 544/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4492 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 545/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4492 - acc: 0.7726 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 546/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4492 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 547/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4491 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 548/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4490 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 549/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4489 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 550/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4489 - acc: 0.7726 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 551/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4488 - acc: 0.7726 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 552/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4488 - acc: 0.7726 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 553/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4487 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 554/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4487 - acc: 0.7726 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 555/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4486 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 556/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4486 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 557/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4485 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 558/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4485 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7604\n",
      "Epoch 559/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4484 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7552\n",
      "Epoch 560/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4483 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7552\n",
      "Epoch 561/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4483 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 562/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4482 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 563/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4482 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 564/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4482 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 565/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4481 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 566/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4480 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 567/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4480 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 568/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4479 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 569/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4479 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 570/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4478 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 571/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4477 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 572/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4477 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 573/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4477 - acc: 0.7743 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 574/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4476 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 575/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4475 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 576/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4475 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 577/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4475 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 578/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4474 - acc: 0.7743 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 579/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4474 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 580/1500\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.4473 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7552\n",
      "Epoch 581/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4473 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 582/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4472 - acc: 0.7743 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 583/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4472 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 584/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4471 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 585/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4471 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 586/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4470 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 587/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4469 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 588/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4469 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 589/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4468 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 590/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4468 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 591/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4467 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 592/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4467 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 593/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4467 - acc: 0.7760 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 594/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4466 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 595/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4466 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7552\n",
      "Epoch 596/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4465 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 597/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4465 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 598/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4464 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 599/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4464 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 600/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4463 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 601/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4463 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 602/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4462 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 603/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4462 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 604/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4461 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 605/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4461 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 606/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4461 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 607/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4460 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 608/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4459 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 609/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4459 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 610/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4458 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7552\n",
      "Epoch 611/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4458 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7552\n",
      "Epoch 612/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4458 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7552\n",
      "Epoch 613/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4457 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7552\n",
      "Epoch 614/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4456 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7552\n",
      "Epoch 615/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4456 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7552\n",
      "Epoch 616/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4456 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7552\n",
      "Epoch 617/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4455 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7552\n",
      "Epoch 618/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4455 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7552\n",
      "Epoch 619/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4455 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7552\n",
      "Epoch 620/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4454 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7552\n",
      "Epoch 621/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4453 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7552\n",
      "Epoch 622/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4453 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7552\n",
      "Epoch 623/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4452 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7552\n",
      "Epoch 624/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4452 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7552\n",
      "Epoch 625/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4451 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7552\n",
      "Epoch 626/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4451 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7552\n",
      "Epoch 627/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4451 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7552\n",
      "Epoch 628/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4450 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7552\n",
      "Epoch 629/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4450 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7552\n",
      "Epoch 630/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4449 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7552\n",
      "Epoch 631/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4449 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7552\n",
      "Epoch 632/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4448 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7552\n",
      "Epoch 633/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4448 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7552\n",
      "Epoch 634/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4447 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7552\n",
      "Epoch 635/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4447 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7552\n",
      "Epoch 636/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4446 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7552\n",
      "Epoch 637/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4446 - acc: 0.7743 - val_loss: 0.4801 - val_acc: 0.7552\n",
      "Epoch 638/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4446 - acc: 0.7760 - val_loss: 0.4801 - val_acc: 0.7552\n",
      "Epoch 639/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4445 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7552\n",
      "Epoch 640/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4445 - acc: 0.7760 - val_loss: 0.4801 - val_acc: 0.7552\n",
      "Epoch 641/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4444 - acc: 0.7760 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 642/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4444 - acc: 0.7760 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 643/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4444 - acc: 0.7760 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 644/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4443 - acc: 0.7743 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 645/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4443 - acc: 0.7760 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 646/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4442 - acc: 0.7743 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 647/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4442 - acc: 0.7743 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 648/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4441 - acc: 0.7743 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 649/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4442 - acc: 0.7743 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 650/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4441 - acc: 0.7743 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 651/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4440 - acc: 0.7760 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 652/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4440 - acc: 0.7743 - val_loss: 0.4802 - val_acc: 0.7552\n",
      "Epoch 653/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4440 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 654/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4439 - acc: 0.7743 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 655/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4439 - acc: 0.7743 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 656/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4438 - acc: 0.7743 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 657/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4438 - acc: 0.7743 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 658/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4438 - acc: 0.7743 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 659/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4437 - acc: 0.7743 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 660/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4437 - acc: 0.7760 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 661/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4436 - acc: 0.7760 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 662/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4436 - acc: 0.7743 - val_loss: 0.4803 - val_acc: 0.7552\n",
      "Epoch 663/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4436 - acc: 0.7743 - val_loss: 0.4804 - val_acc: 0.7552\n",
      "Epoch 664/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4435 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7552\n",
      "Epoch 665/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4435 - acc: 0.7760 - val_loss: 0.4804 - val_acc: 0.7552\n",
      "Epoch 666/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4435 - acc: 0.7760 - val_loss: 0.4804 - val_acc: 0.7552\n",
      "Epoch 667/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4434 - acc: 0.7760 - val_loss: 0.4804 - val_acc: 0.7552\n",
      "Epoch 668/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4434 - acc: 0.7760 - val_loss: 0.4804 - val_acc: 0.7552\n",
      "Epoch 669/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4434 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7552\n",
      "Epoch 670/1500\n",
      " 32/576 [>.............................] - ETA: 0s - loss: 0.3978 - acc: 0.8438"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(6, input_shape=(8,), activation=\"relu\"))\n",
    "model_2.add(Dense(6,  activation=\"relu\"))\n",
    "model_2.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hist_2.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(run_hist_2.history[\"loss\"])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(range(n), (run_hist_2.history[\"loss\"]),'r.', label=\"Train Loss\")\n",
    "ax.plot(range(n), (run_hist_2.history[\"val_loss\"]),'b.', label=\"Validation Loss\")\n",
    "ax.legend()\n",
    "ax.set_title('Loss over iterations')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(range(n), (run_hist_2.history[\"acc\"]),'r.', label=\"Train Acc\")\n",
    "ax.plot(range(n), (run_hist_2.history[\"val_acc\"]),'b.', label=\"Validation Acc\")\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title('Accuracy over iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class_nn_2 = model_2.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_2 = model_2.predict(X_test_norm)\n",
    "print('')\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_2)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_2)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_2, 'NN-2')\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
